{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ALDA_Capstone.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "LY_xDLHOvvii",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Toxic Comment Detection**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Data1: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data **\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Data2: https://www.kaggle.com/c/quora-insincere-questions-classification/data**"
      ]
    },
    {
      "metadata": {
        "id": "MZIxJbbSv1Cg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Import all the required packages and esablish connecion with Google Drive**\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D1QYEn3dsG-s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z96arYWqly9t",
        "colab_type": "code",
        "outputId": "9cdd0492-7e86-4d05-b684-b5a8a2816441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c6pZQF82v6Oc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import data from CSV**"
      ]
    },
    {
      "metadata": {
        "id": "vlZfmyorly9x",
        "colab_type": "code",
        "outputId": "03d74312-8194-43c6-c6bd-8ddeb8af6947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "!cp gdrive/My\\ Drive/ALDA\\ Capstone/train_jigsaw.csv .\n",
        "!ls\n",
        "df = pd.read_csv(\"train_jigsaw.csv\")\n",
        "df = df.dropna()\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cleaned_text.pkl\t\t    gdrive\n",
            "d2v.model\t\t\t    sample_data\n",
            "d2v.model.docvecs.vectors_docs.npy  sequential.pkl\n",
            "d2v.model.trainables.syn1neg.npy    train_jigsaw.csv\n",
            "d2v.model.wv.vectors.npy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
              "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  \n",
              "0             0        0       0       0              0  \n",
              "1             0        0       0       0              0  \n",
              "2             0        0       0       0              0  \n",
              "3             0        0       0       0              0  \n",
              "4             0        0       0       0              0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "Qgpoik6Jv_yf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Update Toxicity**"
      ]
    },
    {
      "metadata": {
        "id": "XDtWNRoIly92",
        "colab_type": "code",
        "outputId": "a7ea9b6d-3e09-4af6-bc9a-1e807cedc41b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df['Toxicity'] = df['toxic'] + df['severe_toxic'] + df['obscene'] + df['threat'] + df['insult'] + df['identity_hate']\n",
        "\n",
        "df.tail(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>Toxicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>159566</th>\n",
              "      <td>ffe987279560d7ff</td>\n",
              "      <td>\":::::And for the second time of asking, when ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159567</th>\n",
              "      <td>ffea4adeee384e90</td>\n",
              "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159568</th>\n",
              "      <td>ffee36eab5c267c9</td>\n",
              "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159569</th>\n",
              "      <td>fff125370e4aaaf3</td>\n",
              "      <td>And it looks like it was actually you who put ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159570</th>\n",
              "      <td>fff46fc426af1f9a</td>\n",
              "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id                                       comment_text  \\\n",
              "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
              "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
              "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
              "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
              "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
              "\n",
              "        toxic  severe_toxic  obscene  threat  insult  identity_hate  Toxicity  \n",
              "159566      0             0        0       0       0              0         0  \n",
              "159567      0             0        0       0       0              0         0  \n",
              "159568      0             0        0       0       0              0         0  \n",
              "159569      0             0        0       0       0              0         0  \n",
              "159570      0             0        0       0       0              0         0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "WwmWymMcly96",
        "colab_type": "code",
        "outputId": "aec6bd30-31f2-4c65-e795-25f9deb8e8c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df.loc[df.Toxicity > 0, ['Toxicity']] = 1\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>Toxicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
              "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
              "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  Toxicity  \n",
              "0             0        0       0       0              0         0  \n",
              "1             0        0       0       0              0         0  \n",
              "2             0        0       0       0              0         0  \n",
              "3             0        0       0       0              0         0  \n",
              "4             0        0       0       0              0         0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "ceFqH71SwGJg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Drop unwanted columns**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "48e44599-2c78-4cd3-a7a4-927f275382da",
        "id": "314uClCO1AB_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "datadf = df[['comment_text','Toxicity']]\n",
        "#datadf = datadf.head(100)                            #333333333333333\n",
        "datadf.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Toxicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text  Toxicity\n",
              "0  Explanation\\nWhy the edits made under my usern...         0\n",
              "1  D'aww! He matches this background colour I'm s...         0\n",
              "2  Hey man, I'm really not trying to edit war. It...         0\n",
              "3  \"\\nMore\\nI can't make any real suggestions on ...         0\n",
              "4  You, sir, are my hero. Any chance you remember...         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "y5D7DKaDwkii",
        "colab_type": "code",
        "outputId": "bc9467ea-be1a-461e-b433-15f2c566cab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "!cp gdrive/My\\ Drive/ALDA\\ Capstone/train.csv .\n",
        "df_quora = pd.read_csv(\"train.csv\")\n",
        "df_quora = df_quora[['question_text', 'target']]\n",
        "df_quora = df_quora.rename(columns={\"question_text\": \"comment_text\", \"target\": \"Toxicity\"})\n",
        "df_quora.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Toxicity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How did Quebec nationalists see their province...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Do you have an adopted dog, how would you enco...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Why does velocity affect time? Does velocity a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text  Toxicity\n",
              "0  How did Quebec nationalists see their province...         0\n",
              "1  Do you have an adopted dog, how would you enco...         0\n",
              "2  Why does velocity affect time? Does velocity a...         0\n",
              "3  How did Otto von Guericke used the Magdeburg h...         0\n",
              "4  Can I convert montra helicon D to a mountain b...         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "TaJZkx8nyayW",
        "colab_type": "code",
        "outputId": "b240bb0b-baef-4161-ff0b-9c4ae5a896bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(datadf))\n",
        "print(len(df_quora))\n",
        "datadf = pd.concat([datadf, df_quora], ignore_index=True, sort=False)\n",
        "datadf = datadf.dropna()\n",
        "print(len(datadf))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159571\n",
            "1306122\n",
            "1465693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "177Rb9v7ly-C",
        "colab_type": "code",
        "outputId": "b629ac21-36ac-4a0a-e7e4-471d077e5499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#data exploration\n",
        "toxic = datadf[datadf.Toxicity ==  1]\n",
        "nontoxic = datadf[datadf.Toxicity ==  0]\n",
        "\n",
        "nontoxic = nontoxic.head(len(toxic)*3)\n",
        "\n",
        "\n",
        "objects = ('Toxic', 'non-Toxic')\n",
        "y_pos = np.arange(len(objects))\n",
        "count = [len(toxic),len(nontoxic)]\n",
        "plt.bar(y_pos, count, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, objects)\n",
        "plt.xlabel('Type ', fontsize=12)\n",
        "plt.ylabel('# of Occurrences', fontsize=12)\n",
        "plt.title(\"# per class\")\n",
        "plt.show()\n",
        "\n",
        "print('Toxic Content->',len(toxic))\n",
        "print('Non-Toxic Content->',len(nontoxic))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEZCAYAAACnyUNvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNCqFJyIiGpXCExERjUrhiYiIRv1/Irt7nqmnTU4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Toxic Content-> 97035\n",
            "Non-Toxic Content-> 291105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "87LnTIaUwRdh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DATA PRE-PROCESSING:**\n",
        "\n",
        "*   Stop words removal\n",
        "*   Stemming\n",
        "*   Cleaning\n",
        "*   Appos handling\n",
        "*   Tokenization\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NO-O5z8uly-F",
        "colab_type": "code",
        "outputId": "a3059fa5-1644-432a-bf92-95eddb1339a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import json\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "!cp gdrive/My\\ Drive/ALDA\\ Capstone/appos.txt .\n",
        "lem = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stop_words = stopwords.words('english')\n",
        "ps = PorterStemmer()\n",
        "appos_words = {}\n",
        "all_words = []\n",
        "\n",
        "with open('appos.txt') as json_file:  \n",
        "    appos_words = json.load(json_file)\n",
        "\n",
        "def processText(text):\n",
        "    \n",
        "    #convert to lower case\n",
        "    \n",
        "    text = text.lower()\n",
        "    \n",
        "    #handle negation \n",
        "    \n",
        "    words = text.split()\n",
        "    text = [appos_words[word] if word in appos_words else word for word in words]\n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    #tokenize words\n",
        "    words = nltk.word_tokenize(text)\n",
        "    \n",
        "    #remove stop words\n",
        "    words  = [i for i in words if i not in stop_words]\n",
        "    \n",
        "    \n",
        "    #Lemmatization\n",
        "    words = [lem.lemmatize(word,\"v\") for word in words]\n",
        "    #stem words\n",
        "    #words = stemSentence(words)\n",
        "\n",
        "    \n",
        "    #remove standalone punctuations\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "    \n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    words = [w.translate(table) for w in words]\n",
        "    \n",
        "    all_words.extend(words)\n",
        "    \n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tVOJBmJWly-J",
        "colab_type": "code",
        "outputId": "69486cf9-c192-4c3d-f697-57caa4b4345c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "datadf['Processed_text'] = datadf.comment_text.apply(processText)\n",
        "\n",
        "datadf.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Toxicity</th>\n",
              "      <th>Processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>[explanation, edit, make, username, hardcore, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>[match, background, colour, I, seemingly, stic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>[hey, man, I, really, try, edit, war, guy, con...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[make, real, suggestions, improvement, wonder,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>[sir, hero, chance, remember, page]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text  Toxicity  \\\n",
              "0  Explanation\\nWhy the edits made under my usern...         0   \n",
              "1  D'aww! He matches this background colour I'm s...         0   \n",
              "2  Hey man, I'm really not trying to edit war. It...         0   \n",
              "3  \"\\nMore\\nI can't make any real suggestions on ...         0   \n",
              "4  You, sir, are my hero. Any chance you remember...         0   \n",
              "\n",
              "                                      Processed_text  \n",
              "0  [explanation, edit, make, username, hardcore, ...  \n",
              "1  [match, background, colour, I, seemingly, stic...  \n",
              "2  [hey, man, I, really, try, edit, war, guy, con...  \n",
              "3  [make, real, suggestions, improvement, wonder,...  \n",
              "4                [sir, hero, chance, remember, page]  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "EDXXwoiIwaZf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Save processed data**"
      ]
    },
    {
      "metadata": {
        "id": "dSwKMPOJ1pKp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_name = 'cleaned_text.pkl'\n",
        "datadf.to_pickle(file_name)\n",
        "!cp ./cleaned_text.pkl gdrive/My\\ Drive/ALDA\\ Capstone/\n",
        "model_dict = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrbvTCYl1rK9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp gdrive/My\\ Drive/ALDA\\ Capstone/cleaned_text.pkl .\n",
        "file_name = 'cleaned_text.pkl'\n",
        "datadf = pd.read_pickle(file_name)\n",
        "model_dict = {}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WGfVQFCM2j1R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datadf.head()\n",
        "\n",
        "\n",
        "#most frequent words\n",
        "fdist = FreqDist(all_words)\n",
        "fdist.plot(30,cumulative=False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fpc0Gjknwea1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create word cloud of toxic and non-toxic separately**"
      ]
    },
    {
      "metadata": {
        "id": "9aDsWjrqRZ81",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "!cp gdrive/My\\ Drive/ALDA\\ Capstone/backimage.jpg .\n",
        "mask = np.array(Image.open(\"backimage.jpg\"))\n",
        "mask = mask[:,:,1]\n",
        "\n",
        "#check frequent words for non - toxic comments in original sentence\n",
        "subset = datadf[datadf.Toxicity ==  0]\n",
        "text = subset.comment_text.values\n",
        "temp = []\n",
        "for words in text:\n",
        "  temp.append(str(words))\n",
        "  \n",
        "text = str(temp)\n",
        "wc = WordCloud(background_color=\"black\",max_words=5000,mask=mask)\n",
        "wc.generate(text)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Words frequented in Non Toxic Comments before processing\", fontsize=20)\n",
        "plt.imshow(wc.recolor(colormap= 'viridis' , random_state=200), alpha=0.98)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#check frequent words for toxic comments\n",
        "subset = datadf[datadf.Toxicity ==  1]\n",
        "text = subset.comment_text.values\n",
        "temp = []\n",
        "for words in text:\n",
        "  temp.append(str(words))\n",
        "  \n",
        "text = str(temp)\n",
        "wc = WordCloud(background_color=\"black\",max_words=5000,mask=mask)\n",
        "wc.generate(text)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Words frequented in Toxic Comments before processing\", fontsize=20)\n",
        "plt.imshow(wc.recolor(colormap= 'Reds' , random_state=2534), alpha=0.98)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#check frequent words for non - toxic comments\n",
        "subset = datadf[datadf.Toxicity ==  0]\n",
        "text = subset.Processed_text.values\n",
        "temp = []\n",
        "for words in text:\n",
        "  temp.append(str(words))\n",
        "  \n",
        "text = str(temp)\n",
        "wc = WordCloud(background_color=\"black\",max_words=5000,mask=mask)\n",
        "wc.generate(text)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Words frequented in Non Toxic Comments\", fontsize=20)\n",
        "plt.imshow(wc.recolor(colormap= 'viridis' , random_state=200), alpha=0.98)\n",
        "plt.show()\n",
        "\n",
        "#check frequent words for toxic comments\n",
        "subset = datadf[datadf.Toxicity ==  1]\n",
        "text = subset.Processed_text.values\n",
        "temp = []\n",
        "for words in text:\n",
        "  temp.append(str(words))\n",
        "  \n",
        "text = str(temp)\n",
        "wc = WordCloud(background_color=\"black\",max_words=5000,mask=mask)\n",
        "wc.generate(text)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Words frequented in Toxic Comments\", fontsize=20)\n",
        "plt.imshow(wc.recolor(colormap= 'Reds' , random_state=2534), alpha=0.98)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LDytiLo1wiZr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Split data into Training dataset and Test dataset separately**"
      ]
    },
    {
      "metadata": {
        "id": "UdyRc-w2AlVT",
        "colab_type": "code",
        "outputId": "3d215af5-cdb2-4506-b675-f8109bb8a080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "y = datadf['Toxicity']\n",
        "x = datadf['Processed_text']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, stratify = y)\n",
        "\n",
        "train_count = y_train.value_counts()\n",
        "test_count = y_test.value_counts()\n",
        "\n",
        "print('Number of Toxic articles in Training set -> ', train_count[0])\n",
        "print('Number of Non Toxic articles in Training set -> ', train_count[1])\n",
        "print('Number of Toxic articles in Testing set -> ', test_count[0])\n",
        "print('Number of Non Toxic articles in Testing set -> ', test_count[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Toxic articles in Training set ->  114676\n",
            "Number of Non Toxic articles in Training set ->  12980\n",
            "Number of Toxic articles in Testing set ->  28670\n",
            "Number of Non Toxic articles in Testing set ->  3245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qladQoyAwm6R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Build and save Doc2Vec model**"
      ]
    },
    {
      "metadata": {
        "id": "CQjvW1SQLPci",
        "colab_type": "code",
        "outputId": "1d9841c5-78a4-480d-ed19-e9d6dc19f70f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "#import multiprocessing\n",
        "#cores = multiprocessing.cpu_count()\n",
        "\n",
        "tagged_data = [TaggedDocument(\n",
        "    words = _d, \n",
        "    tags = [str(i)]) for i, _d in enumerate(x_train)]\n",
        "\n",
        "max_epochs = 30\n",
        "vec_size = 300\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(\n",
        "    vector_size = vec_size,\n",
        "    min_count = 10,\n",
        "    window = 10,\n",
        "    workers = 5,\n",
        "    dm = 1)\n",
        "\n",
        "model.build_vocab(tagged_data)\n",
        "print('Training Doc2Vec Model')\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "    print('Training iteration {0}'.format(epoch + 1))\n",
        "    \n",
        "    model.train(tagged_data,\n",
        "                total_examples = model.corpus_count,\n",
        "                epochs = model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "model.save(\"d2v.model\")\n",
        "model_dict['doc2vec'] = model\n",
        "print(\"Model Saved\")\n",
        "!cp ./d2v.model gdrive/My\\ Drive/ALDA\\ Capstone/\n",
        "!cp ./d2v.model.docvecs.vectors_docs.npy gdrive/My\\ Drive/ALDA\\ Capstone/\n",
        "!cp ./d2v.model.trainables.syn1neg.npy  gdrive/My\\ Drive/ALDA\\ Capstone/\n",
        "!cp ./d2v.model.wv.vectors.npy  gdrive/My\\ Drive/ALDA\\ Capstone/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Doc2Vec Model\n",
            "Training iteration 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training iteration 2\n",
            "Training iteration 3\n",
            "Training iteration 4\n",
            "Training iteration 5\n",
            "Training iteration 6\n",
            "Training iteration 7\n",
            "Training iteration 8\n",
            "Training iteration 9\n",
            "Training iteration 10\n",
            "Training iteration 11\n",
            "Training iteration 12\n",
            "Training iteration 13\n",
            "Training iteration 14\n",
            "Training iteration 15\n",
            "Training iteration 16\n",
            "Training iteration 17\n",
            "Training iteration 18\n",
            "Training iteration 19\n",
            "Training iteration 20\n",
            "Training iteration 21\n",
            "Training iteration 22\n",
            "Training iteration 23\n",
            "Training iteration 24\n",
            "Training iteration 25\n",
            "Training iteration 26\n",
            "Training iteration 27\n",
            "Training iteration 28\n",
            "Training iteration 29\n",
            "Training iteration 30\n",
            "Model Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pM4U768vwqpA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Use Doc2Vec model to create Doc2Vec embeddings for train and test data**"
      ]
    },
    {
      "metadata": {
        "id": "UlVWvGUJPm7H",
        "colab_type": "code",
        "outputId": "5201ece1-781f-4dbe-8c78-4fff13bd8dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "!cp gdrive/My\\ Drive/ALDA\\ Capstone/d2v.model .\n",
        "!cp gdrive/My\\ Drive/ALDA\\ Capstone/d2v.model.docvecs.vectors_docs.npy .\n",
        "!cp gdrive/My\\ Drive/ALDA\\ Capstone/d2v.model.trainables.syn1neg.npy .\n",
        "!cp gdrive/My\\ Drive/ALDA\\ Capstone/d2v.model.wv.vectors.npy .\n",
        "model = Doc2Vec.load(\"d2v.model\")\n",
        "\n",
        "#print(type(x_test))\n",
        "test_data = []\n",
        "for i in range(x_test.shape[0]):\n",
        "    temp = np.concatenate([model.infer_vector(x_test.iloc[i])])\n",
        "    test_data.append(temp)\n",
        "\n",
        "train_data = []\n",
        "\n",
        "for i in range(x_train.shape[0]):\n",
        "    temp = model.docvecs[i]\n",
        "    train_data.append(temp)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qsnoxi6hwuRZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Decision Tree Classifier**"
      ]
    },
    {
      "metadata": {
        "id": "MOJTVdiMnAEN",
        "colab_type": "code",
        "outputId": "6bfbeda6-8dfb-410f-eed2-53aa4c54783a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt_clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth = 22, min_samples_leaf=1)\n",
        "dt_clf.fit(train_data, y_train)\n",
        "\n",
        "dt_y_pred_train = dt_clf.predict(train_data)\n",
        "dt_y_pred_test = dt_clf.predict(test_data)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, dt_y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, dt_y_pred_test)\n",
        "\n",
        "dTree_filename = 'dTree.pkl'\n",
        "# Open the file to save as pkl file\n",
        "decision_tree_model_pkl = open(dTree_filename, 'wb')\n",
        "pickle.dump(dt_clf, decision_tree_model_pkl)\n",
        "# Close the pickle instances\n",
        "decision_tree_model_pkl.close()\n",
        "model_dict['dTree'] = dt_clf\n",
        "print(\"Model Saved\")\n",
        "\n",
        "print('Train Accuracy -> ', train_accuracy*100)\n",
        "print('Test Accuracy -> ', test_accuracy*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Saved\n",
            "Train Accuracy ->  99.5017860500094\n",
            "Test Accuracy ->  82.53172489425036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TcUb7aDqwzIe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Decision Tree Classifier - hyper parameter tuning**"
      ]
    },
    {
      "metadata": {
        "id": "LufakcRVstfm",
        "colab_type": "code",
        "outputId": "34b164c5-e498-405d-8a17-dd8a036af633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_curve, auc , recall_score\n",
        "\n",
        "max_depths = [15,20,23,25,27,30]\n",
        "train_recalls = []\n",
        "train_accuracy = []\n",
        "test_recalls = []\n",
        "test_accuracy = []\n",
        "dtreemodels = {}\n",
        "for max_depth in max_depths:\n",
        "  print(max_depth)\n",
        "  dt_clf = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=max_depth, min_samples_leaf=1)\n",
        "  dt_clf.fit(train_data, y_train)\n",
        "  \n",
        "  dt_y_pred_train = dt_clf.predict(train_data)\n",
        "  dt_y_pred_test = dt_clf.predict(test_data)\n",
        "\n",
        "  train_accuracy.append(accuracy_score(y_train, dt_y_pred_train))\n",
        "  test_accuracy.append(accuracy_score(y_test, dt_y_pred_test))\n",
        "  train_recalls.append(recall_score(y_train, dt_y_pred_train))\n",
        "  test_recalls.append(recall_score(y_test, dt_y_pred_test))\n",
        "  dtreemodels[max_depth] = dt_clf\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n",
            "20\n",
            "23\n",
            "25\n",
            "27\n",
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pCGnP8IYw6KF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Decision Tree Classifier -  plot the parameter graph for train and test recall **"
      ]
    },
    {
      "metadata": {
        "id": "trcAnFwgmBLM",
        "colab_type": "code",
        "outputId": "81f261a5-0cda-409a-84d2-f49d97610113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "line1, = plt.plot(max_depths, train_recalls, 'b', label=\"Train recall\")\n",
        "line2, = plt.plot(max_depths, test_recalls, 'r', label=\"Test recall\")\n",
        "line3, = plt.plot(max_depths, train_accuracy, 'y', label=\"Train accuracy\")\n",
        "line4, = plt.plot(max_depths, test_accuracy, 'g', label=\"Test Accuracy\")\n",
        "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "plt.ylabel('Measure')\n",
        "plt.xlabel('Tree depth')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAENCAYAAAAVPvJNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LnzofOdBQQX4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_dict = {}\n",
        "model_dict['dtree'] = dtreemodels[20]\n",
        "dt_clf = dtreemodels[20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mVCNy3l7xDIV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Gaussian Naive Bayes Classifier**"
      ]
    },
    {
      "metadata": {
        "id": "6htBYadoY1Pd",
        "colab_type": "code",
        "outputId": "2233eb29-c85a-474e-8070-7ca04354f80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(train_data, y_train)\n",
        "\n",
        "nb_y_pred_train = nb.predict(train_data)\n",
        "nb_y_pred_test = nb.predict(test_data)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, nb_y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, nb_y_pred_test)\n",
        "\n",
        "nb_filename = 'nb.pkl'\n",
        "nb_model_pkl = open(nb_filename, 'wb')\n",
        "pickle.dump(nb, nb_model_pkl)\n",
        "nb_model_pkl.close()\n",
        "model_dict['nb'] = nb\n",
        "print(\"Model Saved\")\n",
        "\n",
        "print('Train Accuracy -> ', train_accuracy*100)\n",
        "print('Test Accuracy -> ', test_accuracy*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Saved\n",
            "Train Accuracy ->  47.57551544776587\n",
            "Test Accuracy ->  10.145699514334952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q5IqqsDExGS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Classifier**"
      ]
    },
    {
      "metadata": {
        "id": "YD1Z9tNG1cH6",
        "colab_type": "code",
        "outputId": "6849919e-fc0e-41f9-c52c-eebbb1ee3303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(random_state=0, solver='sag', multi_class='auto').fit(train_data, y_train)\n",
        "\n",
        "lr_y_pred_train = lr.predict(train_data)\n",
        "lr_y_pred_test = lr.predict(test_data)\n",
        "\n",
        "lr_filename = 'lr.pkl'\n",
        "lr_model_pkl = open(lr_filename, 'wb')\n",
        "pickle.dump(lr, lr_model_pkl)\n",
        "lr_model_pkl.close()\n",
        "model_dict['lr'] = lr\n",
        "print(\"Model Saved\")\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, lr_y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, lr_y_pred_test)\n",
        "\n",
        "print('Train Accuracy -> ', train_accuracy*100)\n",
        "print('Test Accuracy -> ', test_accuracy*100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Saved\n",
            "Train Accuracy ->  92.80253180422385\n",
            "Test Accuracy ->  90.7065643114523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AihlRlM1xJ_J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Classifier -  Paramter Tuning** "
      ]
    },
    {
      "metadata": {
        "id": "CVZRdBK1KMTu",
        "colab_type": "code",
        "outputId": "c962e78e-ae2f-4d8a-a2d5-0d7904e98163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "C_param_range = [0.01,0.1,1,10,100]\n",
        "test_accuracys = []\n",
        "test_recalls = []\n",
        "train_accuracys = []\n",
        "train_recalls = []\n",
        "lr_models = {}\n",
        "for i in C_param_range:\n",
        "  print(i)\n",
        "  lr = LogisticRegression(random_state=0, solver='sag', multi_class='auto',C=i).fit(train_data, y_train)\n",
        "  lr_y_pred_train = lr.predict(train_data)\n",
        "  lr_y_pred_test = lr.predict(test_data)\n",
        "  test_accuracys.append(accuracy_score(y_test, lr_y_pred_test))\n",
        "  test_recalls.append(recall_score(y_test, lr_y_pred_test))\n",
        "  train_accuracys.append(accuracy_score(y_train, lr_y_pred_train))\n",
        "  train_recalls.append(recall_score(y_train, lr_y_pred_train))\n",
        "  lr_models[i] = lr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01\n",
            "0.1\n",
            "1\n",
            "10\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MLDQCJMRLVSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Classifier -  plot the parameter graph for test accuracy and test recall **"
      ]
    },
    {
      "metadata": {
        "id": "tD2wf3gwKwBG",
        "colab_type": "code",
        "outputId": "021bb31f-d037-4dbe-f3be-a4c60162dc70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "line1, = plt.plot(C_param_range, train_recalls, 'b', label=\"Train recall\")\n",
        "line2, = plt.plot(C_param_range, test_recalls, 'r', label=\"Test recall\")\n",
        "line3, = plt.plot(C_param_range, train_accuracys, 'y', label=\"Train accuracy\")\n",
        "line4, = plt.plot(C_param_range, test_accuracys, 'g', label=\"Test Accuracy\")\n",
        "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "plt.ylabel('Measure')\n",
        "plt.xlabel('C range')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAENCAYAAAAVPvJNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERFJYoEgIiJJLBBERCTp/wBpPO+1jZnEigAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "u3_UsKg0Mzfu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_dict = {}\n",
        "model_dict['lr'] = lr_models[10]\n",
        "lr = lr_models[10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Se2zIpRIxQ-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Simple Ensemble Model - Voting Classifier**"
      ]
    },
    {
      "metadata": {
        "id": "PBWBVGmXYwe6",
        "colab_type": "code",
        "outputId": "7c8a92f1-be47-45e4-c283-4c79a9485f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "model = VotingClassifier(estimators=[('lr', lr), ('dt', dt_clf), ('nb', nb)], voting='hard')\n",
        "model.fit(train_data,y_train)\n",
        "\n",
        "ensemble_voting_pred_train = model.predict(train_data)\n",
        "ensemble_voting_y_pred_test = model.predict(test_data)\n",
        "\n",
        "ensemble_vote_filename = 'ensemble_vote.pkl'\n",
        "ensemble_vote_model_pkl = open(ensemble_vote_filename, 'wb')\n",
        "pickle.dump(model, ensemble_vote_model_pkl)\n",
        "ensemble_vote_model_pkl.close()\n",
        "model_dict['ensemble_vote'] = model\n",
        "print(\"Model Saved\")\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, ensemble_voting_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, ensemble_voting_y_pred_test)\n",
        "\n",
        "print('Train Accuracy -> {}'.format(train_accuracy*100))\n",
        "print('Test Accuracy -> {}'.format(test_accuracy*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Saved\n",
            "Train Accuracy -> 98.50770821583005\n",
            "Test Accuracy -> 78.19520601597995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MLeGQGpaxUom",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Simple Ensemble Models - Average and Weighted Average Classifiers**"
      ]
    },
    {
      "metadata": {
        "id": "QrIZEr7jZHgh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "average_ensemble_pred_test = (dt_y_pred_test+lr_y_pred_test + nb_y_pred_test)/3\n",
        "average_ensemble_pred_test = average_ensemble_pred_test.astype(int)\n",
        "test_accuracy_average = accuracy_score(y_test, average_ensemble_pred_test)\n",
        "\n",
        "\n",
        "weighted_average_ensemble_pred_test = (dt_y_pred_test*0.25+lr_y_pred_test*0.5+nb_y_pred_test*0.25)\n",
        "weighted_average_ensemble_pred_test = weighted_average_ensemble_pred_test.astype(int)\n",
        "test_accuracy_weighted = accuracy_score(y_test, weighted_average_ensemble_pred_test)\n",
        "\n",
        "print('Average Ensemble Test Accuracy -> {}'.format(test_accuracy_average*100))\n",
        "print('Weighted Average Ensemble Test Accuracy -> {}'.format(test_accuracy_weighted*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "stNuhOwkxYtd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DNN Classifier**"
      ]
    },
    {
      "metadata": {
        "id": "-JarZsKKyiy4",
        "colab_type": "code",
        "outputId": "74aba357-dc17-4745-cb12-ebb7fe370776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5239
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "K.clear_session()\n",
        "tf.reset_default_graph()\n",
        "input_dim   = len(train_data[0])\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='mse', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n",
        "history = model.fit(np.array(train_data), y_train,\n",
        "                    epochs=140,\n",
        "                    verbose=True,\n",
        "                    validation_split=0.20,\n",
        "                    batch_size=64)\n",
        "\n",
        "sequential_filename = 'sequential.pkl'\n",
        "sequential_model_pkl = open(sequential_filename, 'wb')\n",
        "pickle.dump(model, sequential_model_pkl)\n",
        "sequential_model_pkl.close()\n",
        "model_dict['sequential'] = model\n",
        "print(\"Model Saved\")\n",
        "\n",
        "!cp ./sequential.pkl gdrive/My\\ Drive/ALDA\\ Capstone/\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(np.array(test_data), y_test, verbose=True)\n",
        "print(\"Test Accuracy: {:.4f}\".format(test_accuracy))\n",
        "train_loss, train_accuracy = model.evaluate(np.array(train_data), y_train, verbose=True)\n",
        "print(\"Train Accuracy: {:.4f}\".format(train_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 31,121\n",
            "Trainable params: 31,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/140\n",
            "102124/102124 [==============================] - 8s 79us/step - loss: 0.0537 - acc: 0.9337 - val_loss: 0.0517 - val_acc: 0.9373\n",
            "Epoch 2/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0443 - acc: 0.9452 - val_loss: 0.0497 - val_acc: 0.9380\n",
            "Epoch 3/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0391 - acc: 0.9528 - val_loss: 0.0500 - val_acc: 0.9386\n",
            "Epoch 4/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0351 - acc: 0.9583 - val_loss: 0.0503 - val_acc: 0.9389\n",
            "Epoch 5/140\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0318 - acc: 0.9630 - val_loss: 0.0508 - val_acc: 0.9391\n",
            "Epoch 6/140\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0287 - acc: 0.9668 - val_loss: 0.0526 - val_acc: 0.9371\n",
            "Epoch 7/140\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0272 - acc: 0.9688 - val_loss: 0.0543 - val_acc: 0.9356\n",
            "Epoch 8/140\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0253 - acc: 0.9715 - val_loss: 0.0566 - val_acc: 0.9324\n",
            "Epoch 9/140\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0238 - acc: 0.9734 - val_loss: 0.0576 - val_acc: 0.9320\n",
            "Epoch 10/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0227 - acc: 0.9748 - val_loss: 0.0558 - val_acc: 0.9366\n",
            "Epoch 11/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0221 - acc: 0.9755 - val_loss: 0.0557 - val_acc: 0.9367\n",
            "Epoch 12/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0213 - acc: 0.9766 - val_loss: 0.0562 - val_acc: 0.9366\n",
            "Epoch 13/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0203 - acc: 0.9780 - val_loss: 0.0584 - val_acc: 0.9331\n",
            "Epoch 14/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0203 - acc: 0.9780 - val_loss: 0.0561 - val_acc: 0.9364\n",
            "Epoch 15/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0196 - acc: 0.9786 - val_loss: 0.0578 - val_acc: 0.9346\n",
            "Epoch 16/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0190 - acc: 0.9796 - val_loss: 0.0576 - val_acc: 0.9355\n",
            "Epoch 17/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0184 - acc: 0.9802 - val_loss: 0.0576 - val_acc: 0.9361\n",
            "Epoch 18/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0186 - acc: 0.9799 - val_loss: 0.0584 - val_acc: 0.9354\n",
            "Epoch 19/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0181 - acc: 0.9804 - val_loss: 0.0578 - val_acc: 0.9363\n",
            "Epoch 20/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0180 - acc: 0.9807 - val_loss: 0.0595 - val_acc: 0.9333\n",
            "Epoch 21/140\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0176 - acc: 0.9811 - val_loss: 0.0584 - val_acc: 0.9364\n",
            "Epoch 22/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0171 - acc: 0.9818 - val_loss: 0.0596 - val_acc: 0.9353\n",
            "Epoch 23/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0173 - acc: 0.9815 - val_loss: 0.0594 - val_acc: 0.9356\n",
            "Epoch 24/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0173 - acc: 0.9814 - val_loss: 0.0583 - val_acc: 0.9360\n",
            "Epoch 25/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0166 - acc: 0.9824 - val_loss: 0.0632 - val_acc: 0.9297\n",
            "Epoch 26/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0168 - acc: 0.9820 - val_loss: 0.0619 - val_acc: 0.9323\n",
            "Epoch 27/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0166 - acc: 0.9824 - val_loss: 0.0606 - val_acc: 0.9338\n",
            "Epoch 28/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0163 - acc: 0.9827 - val_loss: 0.0633 - val_acc: 0.9312\n",
            "Epoch 29/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0161 - acc: 0.9830 - val_loss: 0.0611 - val_acc: 0.9340\n",
            "Epoch 30/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0157 - acc: 0.9835 - val_loss: 0.0611 - val_acc: 0.9340\n",
            "Epoch 31/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0157 - acc: 0.9834 - val_loss: 0.0635 - val_acc: 0.9301\n",
            "Epoch 32/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0158 - acc: 0.9831 - val_loss: 0.0599 - val_acc: 0.9352\n",
            "Epoch 33/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0162 - acc: 0.9828 - val_loss: 0.0606 - val_acc: 0.9349\n",
            "Epoch 34/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0155 - acc: 0.9837 - val_loss: 0.0599 - val_acc: 0.9360\n",
            "Epoch 35/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0156 - acc: 0.9836 - val_loss: 0.0603 - val_acc: 0.9351\n",
            "Epoch 36/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0155 - acc: 0.9839 - val_loss: 0.0604 - val_acc: 0.9356\n",
            "Epoch 37/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0157 - acc: 0.9834 - val_loss: 0.0603 - val_acc: 0.9356\n",
            "Epoch 38/140\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0152 - acc: 0.9840 - val_loss: 0.0611 - val_acc: 0.9349\n",
            "Epoch 39/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0151 - acc: 0.9842 - val_loss: 0.0628 - val_acc: 0.9325\n",
            "Epoch 40/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9844 - val_loss: 0.0619 - val_acc: 0.9339\n",
            "Epoch 41/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0151 - acc: 0.9842 - val_loss: 0.0619 - val_acc: 0.9337\n",
            "Epoch 42/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0594 - val_acc: 0.9367\n",
            "Epoch 43/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0149 - acc: 0.9844 - val_loss: 0.0606 - val_acc: 0.9363\n",
            "Epoch 44/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0151 - acc: 0.9842 - val_loss: 0.0633 - val_acc: 0.9325\n",
            "Epoch 45/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0620 - val_acc: 0.9335\n",
            "Epoch 46/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0147 - acc: 0.9847 - val_loss: 0.0606 - val_acc: 0.9357\n",
            "Epoch 47/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0148 - acc: 0.9846 - val_loss: 0.0626 - val_acc: 0.9332\n",
            "Epoch 48/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0143 - acc: 0.9852 - val_loss: 0.0609 - val_acc: 0.9353\n",
            "Epoch 49/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0144 - acc: 0.9850 - val_loss: 0.0636 - val_acc: 0.9323\n",
            "Epoch 50/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0617 - val_acc: 0.9347\n",
            "Epoch 51/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0146 - acc: 0.9849 - val_loss: 0.0633 - val_acc: 0.9326\n",
            "Epoch 52/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0147 - acc: 0.9847 - val_loss: 0.0614 - val_acc: 0.9356\n",
            "Epoch 53/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0633 - val_acc: 0.9331\n",
            "Epoch 54/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0144 - acc: 0.9851 - val_loss: 0.0634 - val_acc: 0.9330\n",
            "Epoch 55/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0141 - acc: 0.9854 - val_loss: 0.0620 - val_acc: 0.9347\n",
            "Epoch 56/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0147 - acc: 0.9847 - val_loss: 0.0621 - val_acc: 0.9353\n",
            "Epoch 57/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0144 - acc: 0.9851 - val_loss: 0.0631 - val_acc: 0.9335\n",
            "Epoch 58/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0613 - val_acc: 0.9351\n",
            "Epoch 59/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0141 - acc: 0.9853 - val_loss: 0.0613 - val_acc: 0.9353\n",
            "Epoch 60/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0142 - acc: 0.9853 - val_loss: 0.0618 - val_acc: 0.9349\n",
            "Epoch 61/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0139 - acc: 0.9857 - val_loss: 0.0641 - val_acc: 0.9319\n",
            "Epoch 62/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0141 - acc: 0.9855 - val_loss: 0.0638 - val_acc: 0.9328\n",
            "Epoch 63/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0607 - val_acc: 0.9365\n",
            "Epoch 64/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0609 - val_acc: 0.9367\n",
            "Epoch 65/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0141 - acc: 0.9854 - val_loss: 0.0633 - val_acc: 0.9328\n",
            "Epoch 66/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9855 - val_loss: 0.0621 - val_acc: 0.9351\n",
            "Epoch 67/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0137 - acc: 0.9861 - val_loss: 0.0624 - val_acc: 0.9350\n",
            "Epoch 68/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0142 - acc: 0.9853 - val_loss: 0.0630 - val_acc: 0.9339\n",
            "Epoch 69/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0142 - acc: 0.9853 - val_loss: 0.0625 - val_acc: 0.9342\n",
            "Epoch 70/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0134 - acc: 0.9864 - val_loss: 0.0624 - val_acc: 0.9346\n",
            "Epoch 71/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0137 - acc: 0.9859 - val_loss: 0.0630 - val_acc: 0.9344\n",
            "Epoch 72/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0139 - acc: 0.9857 - val_loss: 0.0610 - val_acc: 0.9363\n",
            "Epoch 73/140\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0136 - acc: 0.9861 - val_loss: 0.0627 - val_acc: 0.9343\n",
            "Epoch 74/140\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0135 - acc: 0.9861 - val_loss: 0.0615 - val_acc: 0.9357\n",
            "Epoch 75/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0138 - acc: 0.9857 - val_loss: 0.0599 - val_acc: 0.9374\n",
            "Epoch 76/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0138 - acc: 0.9859 - val_loss: 0.0615 - val_acc: 0.9363\n",
            "Epoch 77/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0136 - acc: 0.9860 - val_loss: 0.0618 - val_acc: 0.9355\n",
            "Epoch 78/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0135 - acc: 0.9861 - val_loss: 0.0631 - val_acc: 0.9338\n",
            "Epoch 79/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0136 - acc: 0.9860 - val_loss: 0.0635 - val_acc: 0.9337\n",
            "Epoch 80/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0138 - acc: 0.9858 - val_loss: 0.0648 - val_acc: 0.9322\n",
            "Epoch 81/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0138 - acc: 0.9858 - val_loss: 0.0625 - val_acc: 0.9348\n",
            "Epoch 82/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0135 - acc: 0.9861 - val_loss: 0.0621 - val_acc: 0.9356\n",
            "Epoch 83/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0138 - acc: 0.9857 - val_loss: 0.0637 - val_acc: 0.9337\n",
            "Epoch 84/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0135 - acc: 0.9863 - val_loss: 0.0622 - val_acc: 0.9353\n",
            "Epoch 85/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0132 - acc: 0.9865 - val_loss: 0.0624 - val_acc: 0.9350\n",
            "Epoch 86/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0138 - acc: 0.9858 - val_loss: 0.0643 - val_acc: 0.9328\n",
            "Epoch 87/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0629 - val_acc: 0.9343\n",
            "Epoch 88/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0133 - acc: 0.9864 - val_loss: 0.0650 - val_acc: 0.9319\n",
            "Epoch 89/140\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0645 - val_acc: 0.9324\n",
            "Epoch 90/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0133 - acc: 0.9864 - val_loss: 0.0637 - val_acc: 0.9339\n",
            "Epoch 91/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0675 - val_acc: 0.9296\n",
            "Epoch 92/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0132 - acc: 0.9864 - val_loss: 0.0642 - val_acc: 0.9331\n",
            "Epoch 93/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0134 - acc: 0.9863 - val_loss: 0.0639 - val_acc: 0.9337\n",
            "Epoch 94/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0138 - acc: 0.9858 - val_loss: 0.0660 - val_acc: 0.9306\n",
            "Epoch 95/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0637 - val_acc: 0.9336\n",
            "Epoch 96/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0622 - val_acc: 0.9351\n",
            "Epoch 97/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0134 - acc: 0.9864 - val_loss: 0.0634 - val_acc: 0.9342\n",
            "Epoch 98/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0634 - val_acc: 0.9344\n",
            "Epoch 99/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0132 - acc: 0.9864 - val_loss: 0.0653 - val_acc: 0.9317\n",
            "Epoch 100/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0629 - val_acc: 0.9348\n",
            "Epoch 101/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0136 - acc: 0.9861 - val_loss: 0.0630 - val_acc: 0.9346\n",
            "Epoch 102/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0630 - val_acc: 0.9348\n",
            "Epoch 103/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0623 - val_acc: 0.9349\n",
            "Epoch 104/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0630 - val_acc: 0.9346\n",
            "Epoch 105/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0130 - acc: 0.9868 - val_loss: 0.0631 - val_acc: 0.9347\n",
            "Epoch 106/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0132 - acc: 0.9866 - val_loss: 0.0623 - val_acc: 0.9358\n",
            "Epoch 107/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0622 - val_acc: 0.9354\n",
            "Epoch 108/140\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0632 - val_acc: 0.9345\n",
            "Epoch 109/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0129 - acc: 0.9869 - val_loss: 0.0617 - val_acc: 0.9358\n",
            "Epoch 110/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0630 - val_acc: 0.9346\n",
            "Epoch 111/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0630 - val_acc: 0.9346\n",
            "Epoch 112/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0132 - acc: 0.9866 - val_loss: 0.0684 - val_acc: 0.9288\n",
            "Epoch 113/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0645 - val_acc: 0.9327\n",
            "Epoch 114/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0132 - acc: 0.9865 - val_loss: 0.0633 - val_acc: 0.9345\n",
            "Epoch 115/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0634 - val_acc: 0.9340\n",
            "Epoch 116/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0637 - val_acc: 0.9343\n",
            "Epoch 117/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0131 - acc: 0.9865 - val_loss: 0.0635 - val_acc: 0.9342\n",
            "Epoch 118/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0642 - val_acc: 0.9333\n",
            "Epoch 119/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0645 - val_acc: 0.9333\n",
            "Epoch 120/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0649 - val_acc: 0.9326\n",
            "Epoch 121/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0133 - acc: 0.9864 - val_loss: 0.0646 - val_acc: 0.9331\n",
            "Epoch 122/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0125 - acc: 0.9874 - val_loss: 0.0632 - val_acc: 0.9347\n",
            "Epoch 123/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0641 - val_acc: 0.9339\n",
            "Epoch 124/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0129 - acc: 0.9869 - val_loss: 0.0627 - val_acc: 0.9352\n",
            "Epoch 125/140\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0650 - val_acc: 0.9324\n",
            "Epoch 126/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0129 - acc: 0.9869 - val_loss: 0.0664 - val_acc: 0.9312\n",
            "Epoch 127/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0126 - acc: 0.9872 - val_loss: 0.0636 - val_acc: 0.9344\n",
            "Epoch 128/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0128 - acc: 0.9870 - val_loss: 0.0637 - val_acc: 0.9340\n",
            "Epoch 129/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0132 - acc: 0.9865 - val_loss: 0.0636 - val_acc: 0.9339\n",
            "Epoch 130/140\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0131 - acc: 0.9867 - val_loss: 0.0622 - val_acc: 0.9358\n",
            "Epoch 131/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0648 - val_acc: 0.9325\n",
            "Epoch 132/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0654 - val_acc: 0.9319\n",
            "Epoch 133/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0624 - val_acc: 0.9355\n",
            "Epoch 134/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0648 - val_acc: 0.9331\n",
            "Epoch 135/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0126 - acc: 0.9872 - val_loss: 0.0638 - val_acc: 0.9336\n",
            "Epoch 136/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0126 - acc: 0.9872 - val_loss: 0.0628 - val_acc: 0.9348\n",
            "Epoch 137/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0636 - val_acc: 0.9341\n",
            "Epoch 138/140\n",
            "102124/102124 [==============================] - 5s 45us/step - loss: 0.0127 - acc: 0.9871 - val_loss: 0.0622 - val_acc: 0.9358\n",
            "Epoch 139/140\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0129 - acc: 0.9869 - val_loss: 0.0668 - val_acc: 0.9308\n",
            "Epoch 140/140\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0129 - acc: 0.9869 - val_loss: 0.0643 - val_acc: 0.9338\n",
            "Model Saved\n",
            "31915/31915 [==============================] - 1s 34us/step\n",
            "Test Accuracy: 0.8969\n",
            "127656/127656 [==============================] - 4s 33us/step\n",
            "Train Accuracy: 0.9769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FafKC3qrxxYw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DNN Classifier - plot history of training the Dnn model**"
      ]
    },
    {
      "metadata": {
        "id": "7MhpXQRrCUdd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history):\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training acc')\n",
        "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "    \n",
        "plot_history(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vcLCK1cXxcJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DNN Classifier - Paramter Tuning**"
      ]
    },
    {
      "metadata": {
        "id": "Yr56Xy79VQ4g",
        "colab_type": "code",
        "outputId": "8d197fa3-c37b-4686-d96d-d2c3c08a8550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 41072
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "epoch_sizes = [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150]\n",
        "\n",
        "test_recalls = []\n",
        "test_accuracys = []\n",
        "test_conf_matrix = []\n",
        "dnn_models = {}\n",
        "def get_recall(model,epoch):\n",
        "  y_pred_test = model.predict(np.array(test_data))\n",
        "  y_pred = []\n",
        "  for x in y_pred_test:\n",
        "    if x[0] > 0.5:\n",
        "      y_pred.append(1)\n",
        "    else:\n",
        "      y_pred.append(0)\n",
        "  y_pred_test = pd.Series(y_pred)\n",
        "  cm=metrics.confusion_matrix(y_test, y_pred_test)\n",
        "  test_conf_matrix.append(cm)\n",
        "  dnn_models[epo] = model\n",
        "  recall_confusion_model = (cm[1][1])/(cm[1][0]+cm[1][1])\n",
        "  accuracy_confusion_model = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0])\n",
        "  return recall_confusion_model , accuracy_confusion_model\n",
        "\n",
        "for epo in epoch_sizes:\n",
        "  input_dim   = len(train_data[0])\n",
        "  model = Sequential()\n",
        "  model.add(layers.Dense(100, input_dim=input_dim, activation='relu'))\n",
        "  model.add(layers.Dense(10, activation='relu'))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='mse', \n",
        "                optimizer='adam', \n",
        "                metrics=['accuracy'])\n",
        "  model.fit(np.array(train_data), y_train,\n",
        "                      epochs=epo,\n",
        "                      verbose=True,\n",
        "                      validation_split=0.20,\n",
        "                      batch_size=64)\n",
        "  recall , accuracy = get_recall(model,epo)\n",
        "  test_recalls.append(recall)\n",
        "  test_accuracys.append(accuracy)\n",
        "  K.clear_session()\n",
        "  tf.reset_default_graph()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/10\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0556 - acc: 0.9303 - val_loss: 0.0513 - val_acc: 0.9367\n",
            "Epoch 2/10\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0447 - acc: 0.9448 - val_loss: 0.0496 - val_acc: 0.9389\n",
            "Epoch 3/10\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0398 - acc: 0.9511 - val_loss: 0.0499 - val_acc: 0.9378\n",
            "Epoch 4/10\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0358 - acc: 0.9569 - val_loss: 0.0503 - val_acc: 0.9384\n",
            "Epoch 5/10\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0323 - acc: 0.9620 - val_loss: 0.0499 - val_acc: 0.9396\n",
            "Epoch 6/10\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0295 - acc: 0.9658 - val_loss: 0.0511 - val_acc: 0.9393\n",
            "Epoch 7/10\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0274 - acc: 0.9686 - val_loss: 0.0529 - val_acc: 0.9359\n",
            "Epoch 8/10\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0259 - acc: 0.9705 - val_loss: 0.0531 - val_acc: 0.9378\n",
            "Epoch 9/10\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0247 - acc: 0.9719 - val_loss: 0.0554 - val_acc: 0.9347\n",
            "Epoch 10/10\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0230 - acc: 0.9742 - val_loss: 0.0535 - val_acc: 0.9388\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/20\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0555 - acc: 0.9317 - val_loss: 0.0514 - val_acc: 0.9367\n",
            "Epoch 2/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0450 - acc: 0.9449 - val_loss: 0.0511 - val_acc: 0.9365\n",
            "Epoch 3/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0402 - acc: 0.9516 - val_loss: 0.0506 - val_acc: 0.9381\n",
            "Epoch 4/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0363 - acc: 0.9565 - val_loss: 0.0509 - val_acc: 0.9390\n",
            "Epoch 5/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0325 - acc: 0.9620 - val_loss: 0.0517 - val_acc: 0.9377\n",
            "Epoch 6/20\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0297 - acc: 0.9658 - val_loss: 0.0528 - val_acc: 0.9386\n",
            "Epoch 7/20\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0275 - acc: 0.9689 - val_loss: 0.0537 - val_acc: 0.9378\n",
            "Epoch 8/20\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0262 - acc: 0.9706 - val_loss: 0.0539 - val_acc: 0.9377\n",
            "Epoch 9/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0243 - acc: 0.9730 - val_loss: 0.0558 - val_acc: 0.9350\n",
            "Epoch 10/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0236 - acc: 0.9737 - val_loss: 0.0548 - val_acc: 0.9381\n",
            "Epoch 11/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0224 - acc: 0.9755 - val_loss: 0.0558 - val_acc: 0.9372\n",
            "Epoch 12/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0217 - acc: 0.9762 - val_loss: 0.0590 - val_acc: 0.9318\n",
            "Epoch 13/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0212 - acc: 0.9768 - val_loss: 0.0567 - val_acc: 0.9371\n",
            "Epoch 14/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0207 - acc: 0.9776 - val_loss: 0.0575 - val_acc: 0.9360\n",
            "Epoch 15/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0201 - acc: 0.9783 - val_loss: 0.0568 - val_acc: 0.9370\n",
            "Epoch 16/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0198 - acc: 0.9786 - val_loss: 0.0583 - val_acc: 0.9347\n",
            "Epoch 17/20\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0197 - acc: 0.9788 - val_loss: 0.0565 - val_acc: 0.9381\n",
            "Epoch 18/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0190 - acc: 0.9797 - val_loss: 0.0582 - val_acc: 0.9357\n",
            "Epoch 19/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0182 - acc: 0.9805 - val_loss: 0.0590 - val_acc: 0.9351\n",
            "Epoch 20/20\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0184 - acc: 0.9804 - val_loss: 0.0590 - val_acc: 0.9349\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/30\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0539 - acc: 0.9336 - val_loss: 0.0513 - val_acc: 0.9372\n",
            "Epoch 2/30\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0445 - acc: 0.9450 - val_loss: 0.0498 - val_acc: 0.9382\n",
            "Epoch 3/30\n",
            "102124/102124 [==============================] - 6s 57us/step - loss: 0.0397 - acc: 0.9520 - val_loss: 0.0500 - val_acc: 0.9393\n",
            "Epoch 4/30\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0362 - acc: 0.9570 - val_loss: 0.0517 - val_acc: 0.9355\n",
            "Epoch 5/30\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0326 - acc: 0.9620 - val_loss: 0.0510 - val_acc: 0.9381\n",
            "Epoch 6/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0302 - acc: 0.9654 - val_loss: 0.0523 - val_acc: 0.9385\n",
            "Epoch 7/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0283 - acc: 0.9675 - val_loss: 0.0532 - val_acc: 0.9364\n",
            "Epoch 8/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0263 - acc: 0.9704 - val_loss: 0.0548 - val_acc: 0.9359\n",
            "Epoch 9/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0249 - acc: 0.9720 - val_loss: 0.0549 - val_acc: 0.9360\n",
            "Epoch 10/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0238 - acc: 0.9734 - val_loss: 0.0543 - val_acc: 0.9380\n",
            "Epoch 11/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0228 - acc: 0.9746 - val_loss: 0.0545 - val_acc: 0.9366\n",
            "Epoch 12/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0219 - acc: 0.9757 - val_loss: 0.0541 - val_acc: 0.9391\n",
            "Epoch 13/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0211 - acc: 0.9768 - val_loss: 0.0568 - val_acc: 0.9351\n",
            "Epoch 14/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0211 - acc: 0.9768 - val_loss: 0.0561 - val_acc: 0.9363\n",
            "Epoch 15/30\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0199 - acc: 0.9784 - val_loss: 0.0597 - val_acc: 0.9320\n",
            "Epoch 16/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0196 - acc: 0.9789 - val_loss: 0.0566 - val_acc: 0.9377\n",
            "Epoch 17/30\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0193 - acc: 0.9792 - val_loss: 0.0581 - val_acc: 0.9351\n",
            "Epoch 18/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0187 - acc: 0.9799 - val_loss: 0.0581 - val_acc: 0.9356\n",
            "Epoch 19/30\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0184 - acc: 0.9802 - val_loss: 0.0585 - val_acc: 0.9352\n",
            "Epoch 20/30\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0183 - acc: 0.9804 - val_loss: 0.0583 - val_acc: 0.9359\n",
            "Epoch 21/30\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0175 - acc: 0.9814 - val_loss: 0.0632 - val_acc: 0.9293\n",
            "Epoch 22/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0178 - acc: 0.9810 - val_loss: 0.0590 - val_acc: 0.9347\n",
            "Epoch 23/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0176 - acc: 0.9812 - val_loss: 0.0579 - val_acc: 0.9376\n",
            "Epoch 24/30\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0172 - acc: 0.9819 - val_loss: 0.0593 - val_acc: 0.9357\n",
            "Epoch 25/30\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0173 - acc: 0.9817 - val_loss: 0.0575 - val_acc: 0.9379\n",
            "Epoch 26/30\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0171 - acc: 0.9818 - val_loss: 0.0603 - val_acc: 0.9343\n",
            "Epoch 27/30\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0170 - acc: 0.9820 - val_loss: 0.0581 - val_acc: 0.9377\n",
            "Epoch 28/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0168 - acc: 0.9821 - val_loss: 0.0590 - val_acc: 0.9367\n",
            "Epoch 29/30\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0164 - acc: 0.9826 - val_loss: 0.0601 - val_acc: 0.9346\n",
            "Epoch 30/30\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0165 - acc: 0.9826 - val_loss: 0.0602 - val_acc: 0.9347\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/40\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0538 - acc: 0.9335 - val_loss: 0.0507 - val_acc: 0.9380\n",
            "Epoch 2/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0440 - acc: 0.9458 - val_loss: 0.0502 - val_acc: 0.9384\n",
            "Epoch 3/40\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0392 - acc: 0.9522 - val_loss: 0.0499 - val_acc: 0.9378\n",
            "Epoch 4/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0353 - acc: 0.9576 - val_loss: 0.0506 - val_acc: 0.9374\n",
            "Epoch 5/40\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0320 - acc: 0.9622 - val_loss: 0.0511 - val_acc: 0.9384\n",
            "Epoch 6/40\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0293 - acc: 0.9658 - val_loss: 0.0523 - val_acc: 0.9362\n",
            "Epoch 7/40\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0270 - acc: 0.9690 - val_loss: 0.0520 - val_acc: 0.9380\n",
            "Epoch 8/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0253 - acc: 0.9714 - val_loss: 0.0538 - val_acc: 0.9365\n",
            "Epoch 9/40\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0241 - acc: 0.9728 - val_loss: 0.0530 - val_acc: 0.9375\n",
            "Epoch 10/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0228 - acc: 0.9743 - val_loss: 0.0546 - val_acc: 0.9377\n",
            "Epoch 11/40\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0221 - acc: 0.9753 - val_loss: 0.0554 - val_acc: 0.9363\n",
            "Epoch 12/40\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0215 - acc: 0.9760 - val_loss: 0.0556 - val_acc: 0.9368\n",
            "Epoch 13/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0204 - acc: 0.9776 - val_loss: 0.0572 - val_acc: 0.9355\n",
            "Epoch 14/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0203 - acc: 0.9776 - val_loss: 0.0558 - val_acc: 0.9378\n",
            "Epoch 15/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0194 - acc: 0.9788 - val_loss: 0.0595 - val_acc: 0.9320\n",
            "Epoch 16/40\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0192 - acc: 0.9792 - val_loss: 0.0574 - val_acc: 0.9357\n",
            "Epoch 17/40\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0182 - acc: 0.9805 - val_loss: 0.0569 - val_acc: 0.9364\n",
            "Epoch 18/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0182 - acc: 0.9803 - val_loss: 0.0589 - val_acc: 0.9342\n",
            "Epoch 19/40\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0182 - acc: 0.9805 - val_loss: 0.0584 - val_acc: 0.9352\n",
            "Epoch 20/40\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0181 - acc: 0.9804 - val_loss: 0.0574 - val_acc: 0.9366\n",
            "Epoch 21/40\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0175 - acc: 0.9812 - val_loss: 0.0581 - val_acc: 0.9370\n",
            "Epoch 22/40\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0171 - acc: 0.9817 - val_loss: 0.0583 - val_acc: 0.9358\n",
            "Epoch 23/40\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0172 - acc: 0.9816 - val_loss: 0.0592 - val_acc: 0.9348\n",
            "Epoch 24/40\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0168 - acc: 0.9821 - val_loss: 0.0596 - val_acc: 0.9353\n",
            "Epoch 25/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0167 - acc: 0.9822 - val_loss: 0.0594 - val_acc: 0.9346\n",
            "Epoch 26/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0167 - acc: 0.9823 - val_loss: 0.0599 - val_acc: 0.9352\n",
            "Epoch 27/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0161 - acc: 0.9829 - val_loss: 0.0587 - val_acc: 0.9369\n",
            "Epoch 28/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0163 - acc: 0.9827 - val_loss: 0.0596 - val_acc: 0.9355\n",
            "Epoch 29/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0159 - acc: 0.9830 - val_loss: 0.0592 - val_acc: 0.9358\n",
            "Epoch 30/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0159 - acc: 0.9832 - val_loss: 0.0612 - val_acc: 0.9337\n",
            "Epoch 31/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0158 - acc: 0.9834 - val_loss: 0.0602 - val_acc: 0.9352\n",
            "Epoch 32/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9833 - val_loss: 0.0614 - val_acc: 0.9336\n",
            "Epoch 33/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0156 - acc: 0.9837 - val_loss: 0.0618 - val_acc: 0.9334\n",
            "Epoch 34/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0155 - acc: 0.9836 - val_loss: 0.0617 - val_acc: 0.9329\n",
            "Epoch 35/40\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0156 - acc: 0.9834 - val_loss: 0.0615 - val_acc: 0.9338\n",
            "Epoch 36/40\n",
            "102124/102124 [==============================] - 6s 57us/step - loss: 0.0153 - acc: 0.9840 - val_loss: 0.0609 - val_acc: 0.9342\n",
            "Epoch 37/40\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0155 - acc: 0.9838 - val_loss: 0.0597 - val_acc: 0.9366\n",
            "Epoch 38/40\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0153 - acc: 0.9840 - val_loss: 0.0619 - val_acc: 0.9334\n",
            "Epoch 39/40\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0150 - acc: 0.9844 - val_loss: 0.0608 - val_acc: 0.9349\n",
            "Epoch 40/40\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0155 - acc: 0.9836 - val_loss: 0.0616 - val_acc: 0.9335\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/50\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0558 - acc: 0.9302 - val_loss: 0.0513 - val_acc: 0.9358\n",
            "Epoch 2/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0446 - acc: 0.9445 - val_loss: 0.0499 - val_acc: 0.9385\n",
            "Epoch 3/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0399 - acc: 0.9511 - val_loss: 0.0493 - val_acc: 0.9394\n",
            "Epoch 4/50\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0359 - acc: 0.9568 - val_loss: 0.0502 - val_acc: 0.9382\n",
            "Epoch 5/50\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0325 - acc: 0.9615 - val_loss: 0.0508 - val_acc: 0.9382\n",
            "Epoch 6/50\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0299 - acc: 0.9651 - val_loss: 0.0510 - val_acc: 0.9391\n",
            "Epoch 7/50\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0273 - acc: 0.9686 - val_loss: 0.0535 - val_acc: 0.9347\n",
            "Epoch 8/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0258 - acc: 0.9707 - val_loss: 0.0536 - val_acc: 0.9371\n",
            "Epoch 9/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0243 - acc: 0.9727 - val_loss: 0.0539 - val_acc: 0.9374\n",
            "Epoch 10/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0231 - acc: 0.9741 - val_loss: 0.0555 - val_acc: 0.9356\n",
            "Epoch 11/50\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0222 - acc: 0.9755 - val_loss: 0.0561 - val_acc: 0.9346\n",
            "Epoch 12/50\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0214 - acc: 0.9765 - val_loss: 0.0547 - val_acc: 0.9377\n",
            "Epoch 13/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0208 - acc: 0.9775 - val_loss: 0.0568 - val_acc: 0.9362\n",
            "Epoch 14/50\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0204 - acc: 0.9776 - val_loss: 0.0557 - val_acc: 0.9368\n",
            "Epoch 15/50\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0201 - acc: 0.9779 - val_loss: 0.0575 - val_acc: 0.9356\n",
            "Epoch 16/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0194 - acc: 0.9787 - val_loss: 0.0580 - val_acc: 0.9346\n",
            "Epoch 17/50\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0190 - acc: 0.9797 - val_loss: 0.0571 - val_acc: 0.9371\n",
            "Epoch 18/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0187 - acc: 0.9799 - val_loss: 0.0587 - val_acc: 0.9347\n",
            "Epoch 19/50\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0181 - acc: 0.9805 - val_loss: 0.0583 - val_acc: 0.9354\n",
            "Epoch 20/50\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0182 - acc: 0.9805 - val_loss: 0.0598 - val_acc: 0.9340\n",
            "Epoch 21/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0181 - acc: 0.9806 - val_loss: 0.0589 - val_acc: 0.9360\n",
            "Epoch 22/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0178 - acc: 0.9809 - val_loss: 0.0597 - val_acc: 0.9353\n",
            "Epoch 23/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0177 - acc: 0.9808 - val_loss: 0.0598 - val_acc: 0.9356\n",
            "Epoch 24/50\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0177 - acc: 0.9809 - val_loss: 0.0605 - val_acc: 0.9334\n",
            "Epoch 25/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0170 - acc: 0.9820 - val_loss: 0.0587 - val_acc: 0.9363\n",
            "Epoch 26/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0171 - acc: 0.9818 - val_loss: 0.0576 - val_acc: 0.9378\n",
            "Epoch 27/50\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0169 - acc: 0.9819 - val_loss: 0.0606 - val_acc: 0.9341\n",
            "Epoch 28/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0162 - acc: 0.9827 - val_loss: 0.0590 - val_acc: 0.9362\n",
            "Epoch 29/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0165 - acc: 0.9825 - val_loss: 0.0583 - val_acc: 0.9371\n",
            "Epoch 30/50\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0165 - acc: 0.9825 - val_loss: 0.0592 - val_acc: 0.9355\n",
            "Epoch 31/50\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0162 - acc: 0.9828 - val_loss: 0.0612 - val_acc: 0.9333\n",
            "Epoch 32/50\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0160 - acc: 0.9832 - val_loss: 0.0597 - val_acc: 0.9355\n",
            "Epoch 33/50\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0159 - acc: 0.9831 - val_loss: 0.0626 - val_acc: 0.9322\n",
            "Epoch 34/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0154 - acc: 0.9839 - val_loss: 0.0613 - val_acc: 0.9337\n",
            "Epoch 35/50\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0158 - acc: 0.9834 - val_loss: 0.0618 - val_acc: 0.9337\n",
            "Epoch 36/50\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0161 - acc: 0.9829 - val_loss: 0.0599 - val_acc: 0.9366\n",
            "Epoch 37/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9836 - val_loss: 0.0614 - val_acc: 0.9349\n",
            "Epoch 38/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0155 - acc: 0.9837 - val_loss: 0.0611 - val_acc: 0.9355\n",
            "Epoch 39/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0155 - acc: 0.9836 - val_loss: 0.0616 - val_acc: 0.9347\n",
            "Epoch 40/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9836 - val_loss: 0.0595 - val_acc: 0.9363\n",
            "Epoch 41/50\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0149 - acc: 0.9844 - val_loss: 0.0613 - val_acc: 0.9347\n",
            "Epoch 42/50\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0153 - acc: 0.9841 - val_loss: 0.0607 - val_acc: 0.9353\n",
            "Epoch 43/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9841 - val_loss: 0.0627 - val_acc: 0.9326\n",
            "Epoch 44/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9842 - val_loss: 0.0620 - val_acc: 0.9340\n",
            "Epoch 45/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9844 - val_loss: 0.0620 - val_acc: 0.9338\n",
            "Epoch 46/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0647 - val_acc: 0.9308\n",
            "Epoch 47/50\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0146 - acc: 0.9849 - val_loss: 0.0618 - val_acc: 0.9340\n",
            "Epoch 48/50\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0145 - acc: 0.9849 - val_loss: 0.0654 - val_acc: 0.9291\n",
            "Epoch 49/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0635 - val_acc: 0.9327\n",
            "Epoch 50/50\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9842 - val_loss: 0.0635 - val_acc: 0.9322\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/60\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0549 - acc: 0.9322 - val_loss: 0.0510 - val_acc: 0.9360\n",
            "Epoch 2/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0443 - acc: 0.9443 - val_loss: 0.0503 - val_acc: 0.9375\n",
            "Epoch 3/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0401 - acc: 0.9505 - val_loss: 0.0492 - val_acc: 0.9394\n",
            "Epoch 4/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0361 - acc: 0.9564 - val_loss: 0.0498 - val_acc: 0.9379\n",
            "Epoch 5/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0329 - acc: 0.9606 - val_loss: 0.0530 - val_acc: 0.9346\n",
            "Epoch 6/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0300 - acc: 0.9647 - val_loss: 0.0523 - val_acc: 0.9360\n",
            "Epoch 7/60\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0280 - acc: 0.9675 - val_loss: 0.0548 - val_acc: 0.9328\n",
            "Epoch 8/60\n",
            "102124/102124 [==============================] - 6s 57us/step - loss: 0.0262 - acc: 0.9699 - val_loss: 0.0519 - val_acc: 0.9395\n",
            "Epoch 9/60\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0244 - acc: 0.9725 - val_loss: 0.0540 - val_acc: 0.9363\n",
            "Epoch 10/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0233 - acc: 0.9740 - val_loss: 0.0549 - val_acc: 0.9358\n",
            "Epoch 11/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0224 - acc: 0.9749 - val_loss: 0.0565 - val_acc: 0.9347\n",
            "Epoch 12/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0218 - acc: 0.9757 - val_loss: 0.0559 - val_acc: 0.9368\n",
            "Epoch 13/60\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0206 - acc: 0.9775 - val_loss: 0.0580 - val_acc: 0.9342\n",
            "Epoch 14/60\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0203 - acc: 0.9777 - val_loss: 0.0577 - val_acc: 0.9347\n",
            "Epoch 15/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0199 - acc: 0.9782 - val_loss: 0.0571 - val_acc: 0.9361\n",
            "Epoch 16/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0193 - acc: 0.9792 - val_loss: 0.0573 - val_acc: 0.9360\n",
            "Epoch 17/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0190 - acc: 0.9795 - val_loss: 0.0576 - val_acc: 0.9366\n",
            "Epoch 18/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0187 - acc: 0.9800 - val_loss: 0.0594 - val_acc: 0.9342\n",
            "Epoch 19/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0186 - acc: 0.9797 - val_loss: 0.0591 - val_acc: 0.9349\n",
            "Epoch 20/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0183 - acc: 0.9804 - val_loss: 0.0614 - val_acc: 0.9317\n",
            "Epoch 21/60\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0179 - acc: 0.9807 - val_loss: 0.0607 - val_acc: 0.9332\n",
            "Epoch 22/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0176 - acc: 0.9812 - val_loss: 0.0623 - val_acc: 0.9308\n",
            "Epoch 23/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0178 - acc: 0.9810 - val_loss: 0.0590 - val_acc: 0.9353\n",
            "Epoch 24/60\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0171 - acc: 0.9818 - val_loss: 0.0619 - val_acc: 0.9313\n",
            "Epoch 25/60\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0172 - acc: 0.9817 - val_loss: 0.0633 - val_acc: 0.9302\n",
            "Epoch 26/60\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0167 - acc: 0.9824 - val_loss: 0.0609 - val_acc: 0.9331\n",
            "Epoch 27/60\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0164 - acc: 0.9826 - val_loss: 0.0603 - val_acc: 0.9345\n",
            "Epoch 28/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0163 - acc: 0.9826 - val_loss: 0.0615 - val_acc: 0.9334\n",
            "Epoch 29/60\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0166 - acc: 0.9823 - val_loss: 0.0607 - val_acc: 0.9344\n",
            "Epoch 30/60\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0159 - acc: 0.9832 - val_loss: 0.0615 - val_acc: 0.9332\n",
            "Epoch 31/60\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0157 - acc: 0.9833 - val_loss: 0.0627 - val_acc: 0.9314\n",
            "Epoch 32/60\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0157 - acc: 0.9836 - val_loss: 0.0610 - val_acc: 0.9347\n",
            "Epoch 33/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0160 - acc: 0.9830 - val_loss: 0.0618 - val_acc: 0.9335\n",
            "Epoch 34/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0158 - acc: 0.9831 - val_loss: 0.0612 - val_acc: 0.9346\n",
            "Epoch 35/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9832 - val_loss: 0.0608 - val_acc: 0.9349\n",
            "Epoch 36/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9842 - val_loss: 0.0600 - val_acc: 0.9354\n",
            "Epoch 37/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0153 - acc: 0.9838 - val_loss: 0.0602 - val_acc: 0.9354\n",
            "Epoch 38/60\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0149 - acc: 0.9844 - val_loss: 0.0606 - val_acc: 0.9353\n",
            "Epoch 39/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9840 - val_loss: 0.0618 - val_acc: 0.9333\n",
            "Epoch 40/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9846 - val_loss: 0.0611 - val_acc: 0.9347\n",
            "Epoch 41/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9845 - val_loss: 0.0618 - val_acc: 0.9341\n",
            "Epoch 42/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0147 - acc: 0.9845 - val_loss: 0.0622 - val_acc: 0.9332\n",
            "Epoch 43/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9851 - val_loss: 0.0635 - val_acc: 0.9320\n",
            "Epoch 44/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9847 - val_loss: 0.0595 - val_acc: 0.9374\n",
            "Epoch 45/60\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0143 - acc: 0.9849 - val_loss: 0.0612 - val_acc: 0.9347\n",
            "Epoch 46/60\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0146 - acc: 0.9846 - val_loss: 0.0604 - val_acc: 0.9356\n",
            "Epoch 47/60\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0637 - val_acc: 0.9311\n",
            "Epoch 48/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9851 - val_loss: 0.0622 - val_acc: 0.9337\n",
            "Epoch 49/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9854 - val_loss: 0.0613 - val_acc: 0.9352\n",
            "Epoch 50/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9855 - val_loss: 0.0610 - val_acc: 0.9355\n",
            "Epoch 51/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9854 - val_loss: 0.0627 - val_acc: 0.9329\n",
            "Epoch 52/60\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0143 - acc: 0.9850 - val_loss: 0.0606 - val_acc: 0.9359\n",
            "Epoch 53/60\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0139 - acc: 0.9855 - val_loss: 0.0605 - val_acc: 0.9362\n",
            "Epoch 54/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0612 - val_acc: 0.9350\n",
            "Epoch 55/60\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0138 - acc: 0.9856 - val_loss: 0.0612 - val_acc: 0.9350\n",
            "Epoch 56/60\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0625 - val_acc: 0.9336\n",
            "Epoch 57/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0613 - val_acc: 0.9355\n",
            "Epoch 58/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0614 - val_acc: 0.9357\n",
            "Epoch 59/60\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0137 - acc: 0.9859 - val_loss: 0.0620 - val_acc: 0.9345\n",
            "Epoch 60/60\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0639 - val_acc: 0.9319\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/70\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0536 - acc: 0.9341 - val_loss: 0.0524 - val_acc: 0.9349\n",
            "Epoch 2/70\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0446 - acc: 0.9448 - val_loss: 0.0496 - val_acc: 0.9391\n",
            "Epoch 3/70\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0396 - acc: 0.9525 - val_loss: 0.0505 - val_acc: 0.9393\n",
            "Epoch 4/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0357 - acc: 0.9576 - val_loss: 0.0510 - val_acc: 0.9387\n",
            "Epoch 5/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0324 - acc: 0.9621 - val_loss: 0.0517 - val_acc: 0.9378\n",
            "Epoch 6/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0296 - acc: 0.9657 - val_loss: 0.0531 - val_acc: 0.9367\n",
            "Epoch 7/70\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0274 - acc: 0.9688 - val_loss: 0.0539 - val_acc: 0.9367\n",
            "Epoch 8/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0260 - acc: 0.9704 - val_loss: 0.0540 - val_acc: 0.9379\n",
            "Epoch 9/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0247 - acc: 0.9723 - val_loss: 0.0550 - val_acc: 0.9368\n",
            "Epoch 10/70\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0237 - acc: 0.9735 - val_loss: 0.0543 - val_acc: 0.9381\n",
            "Epoch 11/70\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0225 - acc: 0.9751 - val_loss: 0.0567 - val_acc: 0.9356\n",
            "Epoch 12/70\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0221 - acc: 0.9756 - val_loss: 0.0562 - val_acc: 0.9360\n",
            "Epoch 13/70\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0211 - acc: 0.9770 - val_loss: 0.0583 - val_acc: 0.9335\n",
            "Epoch 14/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0207 - acc: 0.9775 - val_loss: 0.0579 - val_acc: 0.9347\n",
            "Epoch 15/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0203 - acc: 0.9782 - val_loss: 0.0593 - val_acc: 0.9342\n",
            "Epoch 16/70\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0204 - acc: 0.9779 - val_loss: 0.0581 - val_acc: 0.9348\n",
            "Epoch 17/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0201 - acc: 0.9782 - val_loss: 0.0586 - val_acc: 0.9346\n",
            "Epoch 18/70\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0192 - acc: 0.9793 - val_loss: 0.0590 - val_acc: 0.9350\n",
            "Epoch 19/70\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0189 - acc: 0.9800 - val_loss: 0.0599 - val_acc: 0.9344\n",
            "Epoch 20/70\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0188 - acc: 0.9798 - val_loss: 0.0596 - val_acc: 0.9348\n",
            "Epoch 21/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0187 - acc: 0.9801 - val_loss: 0.0601 - val_acc: 0.9338\n",
            "Epoch 22/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0180 - acc: 0.9809 - val_loss: 0.0599 - val_acc: 0.9352\n",
            "Epoch 23/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0183 - acc: 0.9806 - val_loss: 0.0588 - val_acc: 0.9366\n",
            "Epoch 24/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0179 - acc: 0.9810 - val_loss: 0.0607 - val_acc: 0.9340\n",
            "Epoch 25/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0179 - acc: 0.9810 - val_loss: 0.0589 - val_acc: 0.9363\n",
            "Epoch 26/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0175 - acc: 0.9815 - val_loss: 0.0613 - val_acc: 0.9332\n",
            "Epoch 27/70\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0176 - acc: 0.9812 - val_loss: 0.0596 - val_acc: 0.9353\n",
            "Epoch 28/70\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0168 - acc: 0.9822 - val_loss: 0.0587 - val_acc: 0.9367\n",
            "Epoch 29/70\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0169 - acc: 0.9823 - val_loss: 0.0605 - val_acc: 0.9346\n",
            "Epoch 30/70\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0167 - acc: 0.9825 - val_loss: 0.0598 - val_acc: 0.9354\n",
            "Epoch 31/70\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0169 - acc: 0.9822 - val_loss: 0.0615 - val_acc: 0.9340\n",
            "Epoch 32/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0167 - acc: 0.9824 - val_loss: 0.0603 - val_acc: 0.9346\n",
            "Epoch 33/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0161 - acc: 0.9833 - val_loss: 0.0626 - val_acc: 0.9326\n",
            "Epoch 34/70\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0165 - acc: 0.9827 - val_loss: 0.0617 - val_acc: 0.9331\n",
            "Epoch 35/70\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0166 - acc: 0.9825 - val_loss: 0.0594 - val_acc: 0.9367\n",
            "Epoch 36/70\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0162 - acc: 0.9830 - val_loss: 0.0601 - val_acc: 0.9355\n",
            "Epoch 37/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0159 - acc: 0.9835 - val_loss: 0.0607 - val_acc: 0.9351\n",
            "Epoch 38/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0160 - acc: 0.9831 - val_loss: 0.0623 - val_acc: 0.9328\n",
            "Epoch 39/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0161 - acc: 0.9831 - val_loss: 0.0630 - val_acc: 0.9323\n",
            "Epoch 40/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9837 - val_loss: 0.0643 - val_acc: 0.9300\n",
            "Epoch 41/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0159 - acc: 0.9832 - val_loss: 0.0605 - val_acc: 0.9357\n",
            "Epoch 42/70\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0153 - acc: 0.9842 - val_loss: 0.0610 - val_acc: 0.9353\n",
            "Epoch 43/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0156 - acc: 0.9838 - val_loss: 0.0601 - val_acc: 0.9361\n",
            "Epoch 44/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0156 - acc: 0.9837 - val_loss: 0.0619 - val_acc: 0.9341\n",
            "Epoch 45/70\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0156 - acc: 0.9838 - val_loss: 0.0648 - val_acc: 0.9309\n",
            "Epoch 46/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0154 - acc: 0.9840 - val_loss: 0.0619 - val_acc: 0.9344\n",
            "Epoch 47/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0153 - acc: 0.9842 - val_loss: 0.0609 - val_acc: 0.9352\n",
            "Epoch 48/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0153 - acc: 0.9842 - val_loss: 0.0613 - val_acc: 0.9351\n",
            "Epoch 49/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0155 - acc: 0.9839 - val_loss: 0.0630 - val_acc: 0.9329\n",
            "Epoch 50/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0155 - acc: 0.9839 - val_loss: 0.0634 - val_acc: 0.9320\n",
            "Epoch 51/70\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0154 - acc: 0.9840 - val_loss: 0.0612 - val_acc: 0.9351\n",
            "Epoch 52/70\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0151 - acc: 0.9843 - val_loss: 0.0593 - val_acc: 0.9376\n",
            "Epoch 53/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0153 - acc: 0.9841 - val_loss: 0.0620 - val_acc: 0.9335\n",
            "Epoch 54/70\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0151 - acc: 0.9844 - val_loss: 0.0610 - val_acc: 0.9358\n",
            "Epoch 55/70\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0148 - acc: 0.9846 - val_loss: 0.0635 - val_acc: 0.9328\n",
            "Epoch 56/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9846 - val_loss: 0.0616 - val_acc: 0.9346\n",
            "Epoch 57/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9846 - val_loss: 0.0627 - val_acc: 0.9335\n",
            "Epoch 58/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9847 - val_loss: 0.0608 - val_acc: 0.9358\n",
            "Epoch 59/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9843 - val_loss: 0.0638 - val_acc: 0.9327\n",
            "Epoch 60/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9843 - val_loss: 0.0612 - val_acc: 0.9357\n",
            "Epoch 61/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9845 - val_loss: 0.0628 - val_acc: 0.9346\n",
            "Epoch 62/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9851 - val_loss: 0.0617 - val_acc: 0.9354\n",
            "Epoch 63/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9842 - val_loss: 0.0620 - val_acc: 0.9351\n",
            "Epoch 64/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0630 - val_acc: 0.9336\n",
            "Epoch 65/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9849 - val_loss: 0.0632 - val_acc: 0.9334\n",
            "Epoch 66/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0638 - val_acc: 0.9333\n",
            "Epoch 67/70\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0145 - acc: 0.9851 - val_loss: 0.0649 - val_acc: 0.9317\n",
            "Epoch 68/70\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0147 - acc: 0.9849 - val_loss: 0.0644 - val_acc: 0.9323\n",
            "Epoch 69/70\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0148 - acc: 0.9848 - val_loss: 0.0620 - val_acc: 0.9350\n",
            "Epoch 70/70\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9852 - val_loss: 0.0622 - val_acc: 0.9354\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/80\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0560 - acc: 0.9290 - val_loss: 0.0526 - val_acc: 0.9351\n",
            "Epoch 2/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0450 - acc: 0.9437 - val_loss: 0.0502 - val_acc: 0.9387\n",
            "Epoch 3/80\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0403 - acc: 0.9507 - val_loss: 0.0508 - val_acc: 0.9366\n",
            "Epoch 4/80\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0364 - acc: 0.9561 - val_loss: 0.0510 - val_acc: 0.9372\n",
            "Epoch 5/80\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0327 - acc: 0.9614 - val_loss: 0.0518 - val_acc: 0.9368\n",
            "Epoch 6/80\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0297 - acc: 0.9654 - val_loss: 0.0536 - val_acc: 0.9352\n",
            "Epoch 7/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0274 - acc: 0.9687 - val_loss: 0.0542 - val_acc: 0.9355\n",
            "Epoch 8/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0256 - acc: 0.9712 - val_loss: 0.0539 - val_acc: 0.9374\n",
            "Epoch 9/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0239 - acc: 0.9733 - val_loss: 0.0537 - val_acc: 0.9380\n",
            "Epoch 10/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0230 - acc: 0.9746 - val_loss: 0.0555 - val_acc: 0.9354\n",
            "Epoch 11/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0220 - acc: 0.9757 - val_loss: 0.0564 - val_acc: 0.9351\n",
            "Epoch 12/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0213 - acc: 0.9766 - val_loss: 0.0576 - val_acc: 0.9341\n",
            "Epoch 13/80\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0207 - acc: 0.9774 - val_loss: 0.0557 - val_acc: 0.9374\n",
            "Epoch 14/80\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0205 - acc: 0.9775 - val_loss: 0.0564 - val_acc: 0.9372\n",
            "Epoch 15/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0195 - acc: 0.9788 - val_loss: 0.0565 - val_acc: 0.9368\n",
            "Epoch 16/80\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0195 - acc: 0.9788 - val_loss: 0.0565 - val_acc: 0.9369\n",
            "Epoch 17/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0188 - acc: 0.9799 - val_loss: 0.0574 - val_acc: 0.9366\n",
            "Epoch 18/80\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0187 - acc: 0.9799 - val_loss: 0.0575 - val_acc: 0.9371\n",
            "Epoch 19/80\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0187 - acc: 0.9800 - val_loss: 0.0572 - val_acc: 0.9367\n",
            "Epoch 20/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0179 - acc: 0.9808 - val_loss: 0.0599 - val_acc: 0.9335\n",
            "Epoch 21/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0181 - acc: 0.9804 - val_loss: 0.0582 - val_acc: 0.9362\n",
            "Epoch 22/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0178 - acc: 0.9809 - val_loss: 0.0578 - val_acc: 0.9373\n",
            "Epoch 23/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0172 - acc: 0.9817 - val_loss: 0.0577 - val_acc: 0.9370\n",
            "Epoch 24/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0172 - acc: 0.9818 - val_loss: 0.0616 - val_acc: 0.9323\n",
            "Epoch 25/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0173 - acc: 0.9815 - val_loss: 0.0584 - val_acc: 0.9371\n",
            "Epoch 26/80\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0170 - acc: 0.9821 - val_loss: 0.0575 - val_acc: 0.9385\n",
            "Epoch 27/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0171 - acc: 0.9818 - val_loss: 0.0595 - val_acc: 0.9349\n",
            "Epoch 28/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0167 - acc: 0.9824 - val_loss: 0.0591 - val_acc: 0.9353\n",
            "Epoch 29/80\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0164 - acc: 0.9829 - val_loss: 0.0600 - val_acc: 0.9346\n",
            "Epoch 30/80\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0163 - acc: 0.9829 - val_loss: 0.0585 - val_acc: 0.9374\n",
            "Epoch 31/80\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0163 - acc: 0.9829 - val_loss: 0.0596 - val_acc: 0.9351\n",
            "Epoch 32/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0167 - acc: 0.9822 - val_loss: 0.0593 - val_acc: 0.9364\n",
            "Epoch 33/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0166 - acc: 0.9826 - val_loss: 0.0599 - val_acc: 0.9354\n",
            "Epoch 34/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0158 - acc: 0.9834 - val_loss: 0.0608 - val_acc: 0.9350\n",
            "Epoch 35/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0160 - acc: 0.9833 - val_loss: 0.0627 - val_acc: 0.9320\n",
            "Epoch 36/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0160 - acc: 0.9831 - val_loss: 0.0606 - val_acc: 0.9351\n",
            "Epoch 37/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0161 - acc: 0.9831 - val_loss: 0.0601 - val_acc: 0.9359\n",
            "Epoch 38/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9835 - val_loss: 0.0606 - val_acc: 0.9356\n",
            "Epoch 39/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0155 - acc: 0.9837 - val_loss: 0.0595 - val_acc: 0.9367\n",
            "Epoch 40/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0154 - acc: 0.9840 - val_loss: 0.0600 - val_acc: 0.9359\n",
            "Epoch 41/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0153 - acc: 0.9841 - val_loss: 0.0627 - val_acc: 0.9332\n",
            "Epoch 42/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0155 - acc: 0.9837 - val_loss: 0.0623 - val_acc: 0.9334\n",
            "Epoch 43/80\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0150 - acc: 0.9843 - val_loss: 0.0613 - val_acc: 0.9347\n",
            "Epoch 44/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9843 - val_loss: 0.0615 - val_acc: 0.9346\n",
            "Epoch 45/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9848 - val_loss: 0.0608 - val_acc: 0.9353\n",
            "Epoch 46/80\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0153 - acc: 0.9840 - val_loss: 0.0620 - val_acc: 0.9334\n",
            "Epoch 47/80\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0605 - val_acc: 0.9360\n",
            "Epoch 48/80\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0148 - acc: 0.9845 - val_loss: 0.0623 - val_acc: 0.9336\n",
            "Epoch 49/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0144 - acc: 0.9851 - val_loss: 0.0608 - val_acc: 0.9356\n",
            "Epoch 50/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0147 - acc: 0.9848 - val_loss: 0.0639 - val_acc: 0.9322\n",
            "Epoch 51/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9843 - val_loss: 0.0616 - val_acc: 0.9348\n",
            "Epoch 52/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0147 - acc: 0.9847 - val_loss: 0.0618 - val_acc: 0.9345\n",
            "Epoch 53/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0617 - val_acc: 0.9353\n",
            "Epoch 54/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9847 - val_loss: 0.0621 - val_acc: 0.9349\n",
            "Epoch 55/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0621 - val_acc: 0.9346\n",
            "Epoch 56/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9853 - val_loss: 0.0624 - val_acc: 0.9346\n",
            "Epoch 57/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0147 - acc: 0.9848 - val_loss: 0.0628 - val_acc: 0.9335\n",
            "Epoch 58/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9849 - val_loss: 0.0608 - val_acc: 0.9358\n",
            "Epoch 59/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0602 - val_acc: 0.9371\n",
            "Epoch 60/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9855 - val_loss: 0.0616 - val_acc: 0.9352\n",
            "Epoch 61/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0144 - acc: 0.9850 - val_loss: 0.0627 - val_acc: 0.9341\n",
            "Epoch 62/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9851 - val_loss: 0.0602 - val_acc: 0.9371\n",
            "Epoch 63/80\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0139 - acc: 0.9859 - val_loss: 0.0629 - val_acc: 0.9339\n",
            "Epoch 64/80\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0143 - acc: 0.9851 - val_loss: 0.0630 - val_acc: 0.9335\n",
            "Epoch 65/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0141 - acc: 0.9855 - val_loss: 0.0603 - val_acc: 0.9369\n",
            "Epoch 66/80\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0143 - acc: 0.9851 - val_loss: 0.0615 - val_acc: 0.9352\n",
            "Epoch 67/80\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0140 - acc: 0.9855 - val_loss: 0.0620 - val_acc: 0.9345\n",
            "Epoch 68/80\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0140 - acc: 0.9856 - val_loss: 0.0615 - val_acc: 0.9356\n",
            "Epoch 69/80\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0142 - acc: 0.9853 - val_loss: 0.0614 - val_acc: 0.9355\n",
            "Epoch 70/80\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0631 - val_acc: 0.9333\n",
            "Epoch 71/80\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0140 - acc: 0.9856 - val_loss: 0.0607 - val_acc: 0.9362\n",
            "Epoch 72/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9853 - val_loss: 0.0626 - val_acc: 0.9342\n",
            "Epoch 73/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9857 - val_loss: 0.0622 - val_acc: 0.9351\n",
            "Epoch 74/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0141 - acc: 0.9855 - val_loss: 0.0624 - val_acc: 0.9345\n",
            "Epoch 75/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0138 - acc: 0.9858 - val_loss: 0.0608 - val_acc: 0.9367\n",
            "Epoch 76/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0139 - acc: 0.9858 - val_loss: 0.0617 - val_acc: 0.9353\n",
            "Epoch 77/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0139 - acc: 0.9858 - val_loss: 0.0651 - val_acc: 0.9312\n",
            "Epoch 78/80\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9855 - val_loss: 0.0619 - val_acc: 0.9353\n",
            "Epoch 79/80\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0138 - acc: 0.9859 - val_loss: 0.0616 - val_acc: 0.9358\n",
            "Epoch 80/80\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0142 - acc: 0.9855 - val_loss: 0.0601 - val_acc: 0.9370\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/90\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0544 - acc: 0.9331 - val_loss: 0.0521 - val_acc: 0.9365\n",
            "Epoch 2/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0449 - acc: 0.9448 - val_loss: 0.0503 - val_acc: 0.9389\n",
            "Epoch 3/90\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0403 - acc: 0.9511 - val_loss: 0.0496 - val_acc: 0.9393\n",
            "Epoch 4/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0364 - acc: 0.9568 - val_loss: 0.0515 - val_acc: 0.9378\n",
            "Epoch 5/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0326 - acc: 0.9617 - val_loss: 0.0510 - val_acc: 0.9388\n",
            "Epoch 6/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0298 - acc: 0.9656 - val_loss: 0.0526 - val_acc: 0.9378\n",
            "Epoch 7/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0275 - acc: 0.9687 - val_loss: 0.0533 - val_acc: 0.9371\n",
            "Epoch 8/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0260 - acc: 0.9708 - val_loss: 0.0561 - val_acc: 0.9336\n",
            "Epoch 9/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0243 - acc: 0.9728 - val_loss: 0.0541 - val_acc: 0.9382\n",
            "Epoch 10/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0234 - acc: 0.9740 - val_loss: 0.0543 - val_acc: 0.9376\n",
            "Epoch 11/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0225 - acc: 0.9752 - val_loss: 0.0541 - val_acc: 0.9392\n",
            "Epoch 12/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0219 - acc: 0.9758 - val_loss: 0.0561 - val_acc: 0.9373\n",
            "Epoch 13/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0208 - acc: 0.9775 - val_loss: 0.0574 - val_acc: 0.9348\n",
            "Epoch 14/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0206 - acc: 0.9774 - val_loss: 0.0565 - val_acc: 0.9366\n",
            "Epoch 15/90\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0198 - acc: 0.9786 - val_loss: 0.0573 - val_acc: 0.9363\n",
            "Epoch 16/90\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0194 - acc: 0.9792 - val_loss: 0.0587 - val_acc: 0.9352\n",
            "Epoch 17/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0191 - acc: 0.9796 - val_loss: 0.0573 - val_acc: 0.9366\n",
            "Epoch 18/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0189 - acc: 0.9798 - val_loss: 0.0600 - val_acc: 0.9327\n",
            "Epoch 19/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0184 - acc: 0.9803 - val_loss: 0.0590 - val_acc: 0.9344\n",
            "Epoch 20/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0181 - acc: 0.9806 - val_loss: 0.0596 - val_acc: 0.9343\n",
            "Epoch 21/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0184 - acc: 0.9803 - val_loss: 0.0598 - val_acc: 0.9342\n",
            "Epoch 22/90\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0179 - acc: 0.9811 - val_loss: 0.0603 - val_acc: 0.9340\n",
            "Epoch 23/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0178 - acc: 0.9810 - val_loss: 0.0614 - val_acc: 0.9324\n",
            "Epoch 24/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0180 - acc: 0.9808 - val_loss: 0.0600 - val_acc: 0.9344\n",
            "Epoch 25/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0175 - acc: 0.9815 - val_loss: 0.0593 - val_acc: 0.9358\n",
            "Epoch 26/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0174 - acc: 0.9815 - val_loss: 0.0598 - val_acc: 0.9349\n",
            "Epoch 27/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0171 - acc: 0.9819 - val_loss: 0.0586 - val_acc: 0.9373\n",
            "Epoch 28/90\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0168 - acc: 0.9823 - val_loss: 0.0599 - val_acc: 0.9352\n",
            "Epoch 29/90\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0163 - acc: 0.9829 - val_loss: 0.0609 - val_acc: 0.9340\n",
            "Epoch 30/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0168 - acc: 0.9822 - val_loss: 0.0679 - val_acc: 0.9255\n",
            "Epoch 31/90\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0166 - acc: 0.9825 - val_loss: 0.0597 - val_acc: 0.9359\n",
            "Epoch 32/90\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0160 - acc: 0.9832 - val_loss: 0.0598 - val_acc: 0.9364\n",
            "Epoch 33/90\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0159 - acc: 0.9833 - val_loss: 0.0630 - val_acc: 0.9318\n",
            "Epoch 34/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0160 - acc: 0.9833 - val_loss: 0.0603 - val_acc: 0.9355\n",
            "Epoch 35/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0159 - acc: 0.9834 - val_loss: 0.0608 - val_acc: 0.9351\n",
            "Epoch 36/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0158 - acc: 0.9835 - val_loss: 0.0616 - val_acc: 0.9348\n",
            "Epoch 37/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9837 - val_loss: 0.0619 - val_acc: 0.9338\n",
            "Epoch 38/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0161 - acc: 0.9831 - val_loss: 0.0618 - val_acc: 0.9338\n",
            "Epoch 39/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9836 - val_loss: 0.0615 - val_acc: 0.9350\n",
            "Epoch 40/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0158 - acc: 0.9836 - val_loss: 0.0619 - val_acc: 0.9340\n",
            "Epoch 41/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0159 - acc: 0.9834 - val_loss: 0.0598 - val_acc: 0.9372\n",
            "Epoch 42/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9836 - val_loss: 0.0618 - val_acc: 0.9347\n",
            "Epoch 43/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9843 - val_loss: 0.0609 - val_acc: 0.9356\n",
            "Epoch 44/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0154 - acc: 0.9839 - val_loss: 0.0603 - val_acc: 0.9362\n",
            "Epoch 45/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0156 - acc: 0.9837 - val_loss: 0.0615 - val_acc: 0.9347\n",
            "Epoch 46/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0153 - acc: 0.9842 - val_loss: 0.0620 - val_acc: 0.9344\n",
            "Epoch 47/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9844 - val_loss: 0.0626 - val_acc: 0.9335\n",
            "Epoch 48/90\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0615 - val_acc: 0.9347\n",
            "Epoch 49/90\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0151 - acc: 0.9843 - val_loss: 0.0633 - val_acc: 0.9327\n",
            "Epoch 50/90\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0618 - val_acc: 0.9346\n",
            "Epoch 51/90\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0151 - acc: 0.9844 - val_loss: 0.0605 - val_acc: 0.9360\n",
            "Epoch 52/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9846 - val_loss: 0.0635 - val_acc: 0.9321\n",
            "Epoch 53/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0153 - acc: 0.9841 - val_loss: 0.0630 - val_acc: 0.9333\n",
            "Epoch 54/90\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0148 - acc: 0.9847 - val_loss: 0.0611 - val_acc: 0.9360\n",
            "Epoch 55/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0618 - val_acc: 0.9348\n",
            "Epoch 56/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9847 - val_loss: 0.0617 - val_acc: 0.9353\n",
            "Epoch 57/90\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0626 - val_acc: 0.9342\n",
            "Epoch 58/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9849 - val_loss: 0.0624 - val_acc: 0.9341\n",
            "Epoch 59/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0144 - acc: 0.9853 - val_loss: 0.0620 - val_acc: 0.9348\n",
            "Epoch 60/90\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0145 - acc: 0.9851 - val_loss: 0.0648 - val_acc: 0.9312\n",
            "Epoch 61/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0617 - val_acc: 0.9353\n",
            "Epoch 62/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9846 - val_loss: 0.0618 - val_acc: 0.9351\n",
            "Epoch 63/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9852 - val_loss: 0.0629 - val_acc: 0.9334\n",
            "Epoch 64/90\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0622 - val_acc: 0.9347\n",
            "Epoch 65/90\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0144 - acc: 0.9853 - val_loss: 0.0622 - val_acc: 0.9347\n",
            "Epoch 66/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0622 - val_acc: 0.9348\n",
            "Epoch 67/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9851 - val_loss: 0.0620 - val_acc: 0.9348\n",
            "Epoch 68/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0144 - acc: 0.9852 - val_loss: 0.0626 - val_acc: 0.9344\n",
            "Epoch 69/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0626 - val_acc: 0.9342\n",
            "Epoch 70/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9853 - val_loss: 0.0627 - val_acc: 0.9340\n",
            "Epoch 71/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0629 - val_acc: 0.9337\n",
            "Epoch 72/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9856 - val_loss: 0.0621 - val_acc: 0.9345\n",
            "Epoch 73/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0610 - val_acc: 0.9357\n",
            "Epoch 74/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9857 - val_loss: 0.0639 - val_acc: 0.9326\n",
            "Epoch 75/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0139 - acc: 0.9858 - val_loss: 0.0617 - val_acc: 0.9357\n",
            "Epoch 76/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9852 - val_loss: 0.0650 - val_acc: 0.9315\n",
            "Epoch 77/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0141 - acc: 0.9855 - val_loss: 0.0626 - val_acc: 0.9348\n",
            "Epoch 78/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0141 - acc: 0.9856 - val_loss: 0.0619 - val_acc: 0.9353\n",
            "Epoch 79/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0139 - acc: 0.9859 - val_loss: 0.0669 - val_acc: 0.9297\n",
            "Epoch 80/90\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0138 - acc: 0.9858 - val_loss: 0.0616 - val_acc: 0.9360\n",
            "Epoch 81/90\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0141 - acc: 0.9855 - val_loss: 0.0638 - val_acc: 0.9335\n",
            "Epoch 82/90\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0141 - acc: 0.9856 - val_loss: 0.0648 - val_acc: 0.9322\n",
            "Epoch 83/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0616 - val_acc: 0.9357\n",
            "Epoch 84/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0141 - acc: 0.9857 - val_loss: 0.0630 - val_acc: 0.9341\n",
            "Epoch 85/90\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0141 - acc: 0.9856 - val_loss: 0.0627 - val_acc: 0.9347\n",
            "Epoch 86/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9851 - val_loss: 0.0613 - val_acc: 0.9369\n",
            "Epoch 87/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0138 - acc: 0.9858 - val_loss: 0.0639 - val_acc: 0.9333\n",
            "Epoch 88/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0137 - acc: 0.9861 - val_loss: 0.0649 - val_acc: 0.9325\n",
            "Epoch 89/90\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0139 - acc: 0.9858 - val_loss: 0.0644 - val_acc: 0.9325\n",
            "Epoch 90/90\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0140 - acc: 0.9858 - val_loss: 0.0612 - val_acc: 0.9369\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/100\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0546 - acc: 0.9324 - val_loss: 0.0521 - val_acc: 0.9345\n",
            "Epoch 2/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0448 - acc: 0.9447 - val_loss: 0.0504 - val_acc: 0.9386\n",
            "Epoch 3/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0401 - acc: 0.9515 - val_loss: 0.0504 - val_acc: 0.9383\n",
            "Epoch 4/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0359 - acc: 0.9574 - val_loss: 0.0505 - val_acc: 0.9385\n",
            "Epoch 5/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0324 - acc: 0.9622 - val_loss: 0.0515 - val_acc: 0.9374\n",
            "Epoch 6/100\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0294 - acc: 0.9665 - val_loss: 0.0518 - val_acc: 0.9396\n",
            "Epoch 7/100\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0274 - acc: 0.9690 - val_loss: 0.0530 - val_acc: 0.9376\n",
            "Epoch 8/100\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0254 - acc: 0.9714 - val_loss: 0.0529 - val_acc: 0.9383\n",
            "Epoch 9/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0241 - acc: 0.9731 - val_loss: 0.0562 - val_acc: 0.9332\n",
            "Epoch 10/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0232 - acc: 0.9745 - val_loss: 0.0547 - val_acc: 0.9371\n",
            "Epoch 11/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0223 - acc: 0.9752 - val_loss: 0.0580 - val_acc: 0.9331\n",
            "Epoch 12/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0218 - acc: 0.9762 - val_loss: 0.0560 - val_acc: 0.9381\n",
            "Epoch 13/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0210 - acc: 0.9774 - val_loss: 0.0562 - val_acc: 0.9365\n",
            "Epoch 14/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0204 - acc: 0.9780 - val_loss: 0.0563 - val_acc: 0.9371\n",
            "Epoch 15/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0205 - acc: 0.9778 - val_loss: 0.0592 - val_acc: 0.9337\n",
            "Epoch 16/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0199 - acc: 0.9785 - val_loss: 0.0557 - val_acc: 0.9385\n",
            "Epoch 17/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0191 - acc: 0.9796 - val_loss: 0.0574 - val_acc: 0.9364\n",
            "Epoch 18/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0191 - acc: 0.9794 - val_loss: 0.0564 - val_acc: 0.9380\n",
            "Epoch 19/100\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0186 - acc: 0.9801 - val_loss: 0.0570 - val_acc: 0.9370\n",
            "Epoch 20/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0184 - acc: 0.9805 - val_loss: 0.0574 - val_acc: 0.9371\n",
            "Epoch 21/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0184 - acc: 0.9803 - val_loss: 0.0592 - val_acc: 0.9351\n",
            "Epoch 22/100\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0182 - acc: 0.9804 - val_loss: 0.0589 - val_acc: 0.9356\n",
            "Epoch 23/100\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0179 - acc: 0.9811 - val_loss: 0.0603 - val_acc: 0.9342\n",
            "Epoch 24/100\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0177 - acc: 0.9813 - val_loss: 0.0614 - val_acc: 0.9324\n",
            "Epoch 25/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0176 - acc: 0.9812 - val_loss: 0.0595 - val_acc: 0.9347\n",
            "Epoch 26/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0172 - acc: 0.9818 - val_loss: 0.0594 - val_acc: 0.9353\n",
            "Epoch 27/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0171 - acc: 0.9818 - val_loss: 0.0588 - val_acc: 0.9359\n",
            "Epoch 28/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0168 - acc: 0.9824 - val_loss: 0.0579 - val_acc: 0.9374\n",
            "Epoch 29/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0168 - acc: 0.9822 - val_loss: 0.0589 - val_acc: 0.9355\n",
            "Epoch 30/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0163 - acc: 0.9829 - val_loss: 0.0591 - val_acc: 0.9363\n",
            "Epoch 31/100\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0166 - acc: 0.9826 - val_loss: 0.0581 - val_acc: 0.9380\n",
            "Epoch 32/100\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0166 - acc: 0.9825 - val_loss: 0.0601 - val_acc: 0.9353\n",
            "Epoch 33/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0163 - acc: 0.9829 - val_loss: 0.0589 - val_acc: 0.9369\n",
            "Epoch 34/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0162 - acc: 0.9830 - val_loss: 0.0575 - val_acc: 0.9385\n",
            "Epoch 35/100\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0163 - acc: 0.9830 - val_loss: 0.0588 - val_acc: 0.9374\n",
            "Epoch 36/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0162 - acc: 0.9829 - val_loss: 0.0581 - val_acc: 0.9383\n",
            "Epoch 37/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0158 - acc: 0.9837 - val_loss: 0.0588 - val_acc: 0.9374\n",
            "Epoch 38/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0162 - acc: 0.9830 - val_loss: 0.0612 - val_acc: 0.9348\n",
            "Epoch 39/100\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0160 - acc: 0.9832 - val_loss: 0.0605 - val_acc: 0.9350\n",
            "Epoch 40/100\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0160 - acc: 0.9832 - val_loss: 0.0604 - val_acc: 0.9360\n",
            "Epoch 41/100\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0157 - acc: 0.9836 - val_loss: 0.0603 - val_acc: 0.9354\n",
            "Epoch 42/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9836 - val_loss: 0.0607 - val_acc: 0.9355\n",
            "Epoch 43/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0155 - acc: 0.9838 - val_loss: 0.0595 - val_acc: 0.9366\n",
            "Epoch 44/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9842 - val_loss: 0.0604 - val_acc: 0.9355\n",
            "Epoch 45/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9836 - val_loss: 0.0628 - val_acc: 0.9328\n",
            "Epoch 46/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9843 - val_loss: 0.0591 - val_acc: 0.9370\n",
            "Epoch 47/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9844 - val_loss: 0.0624 - val_acc: 0.9333\n",
            "Epoch 48/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0156 - acc: 0.9838 - val_loss: 0.0596 - val_acc: 0.9364\n",
            "Epoch 49/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9842 - val_loss: 0.0610 - val_acc: 0.9347\n",
            "Epoch 50/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9843 - val_loss: 0.0630 - val_acc: 0.9329\n",
            "Epoch 51/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9846 - val_loss: 0.0622 - val_acc: 0.9335\n",
            "Epoch 52/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9842 - val_loss: 0.0616 - val_acc: 0.9345\n",
            "Epoch 53/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9846 - val_loss: 0.0612 - val_acc: 0.9352\n",
            "Epoch 54/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9844 - val_loss: 0.0608 - val_acc: 0.9355\n",
            "Epoch 55/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9846 - val_loss: 0.0602 - val_acc: 0.9363\n",
            "Epoch 56/100\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0147 - acc: 0.9848 - val_loss: 0.0593 - val_acc: 0.9373\n",
            "Epoch 57/100\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0147 - acc: 0.9849 - val_loss: 0.0604 - val_acc: 0.9360\n",
            "Epoch 58/100\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0144 - acc: 0.9851 - val_loss: 0.0615 - val_acc: 0.9353\n",
            "Epoch 59/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9843 - val_loss: 0.0606 - val_acc: 0.9354\n",
            "Epoch 60/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9847 - val_loss: 0.0620 - val_acc: 0.9349\n",
            "Epoch 61/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9847 - val_loss: 0.0621 - val_acc: 0.9340\n",
            "Epoch 62/100\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0145 - acc: 0.9851 - val_loss: 0.0623 - val_acc: 0.9342\n",
            "Epoch 63/100\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0605 - val_acc: 0.9363\n",
            "Epoch 64/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0617 - val_acc: 0.9348\n",
            "Epoch 65/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9856 - val_loss: 0.0619 - val_acc: 0.9346\n",
            "Epoch 66/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9851 - val_loss: 0.0636 - val_acc: 0.9331\n",
            "Epoch 67/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9849 - val_loss: 0.0621 - val_acc: 0.9346\n",
            "Epoch 68/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9850 - val_loss: 0.0625 - val_acc: 0.9346\n",
            "Epoch 69/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9846 - val_loss: 0.0603 - val_acc: 0.9367\n",
            "Epoch 70/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9850 - val_loss: 0.0616 - val_acc: 0.9355\n",
            "Epoch 71/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9854 - val_loss: 0.0617 - val_acc: 0.9347\n",
            "Epoch 72/100\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0618 - val_acc: 0.9350\n",
            "Epoch 73/100\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0645 - val_acc: 0.9320\n",
            "Epoch 74/100\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0143 - acc: 0.9853 - val_loss: 0.0612 - val_acc: 0.9359\n",
            "Epoch 75/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0144 - acc: 0.9853 - val_loss: 0.0629 - val_acc: 0.9339\n",
            "Epoch 76/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9851 - val_loss: 0.0612 - val_acc: 0.9358\n",
            "Epoch 77/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9853 - val_loss: 0.0611 - val_acc: 0.9358\n",
            "Epoch 78/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0141 - acc: 0.9856 - val_loss: 0.0622 - val_acc: 0.9346\n",
            "Epoch 79/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0621 - val_acc: 0.9353\n",
            "Epoch 80/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9849 - val_loss: 0.0633 - val_acc: 0.9332\n",
            "Epoch 81/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9855 - val_loss: 0.0612 - val_acc: 0.9363\n",
            "Epoch 82/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0144 - acc: 0.9852 - val_loss: 0.0606 - val_acc: 0.9364\n",
            "Epoch 83/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9853 - val_loss: 0.0611 - val_acc: 0.9362\n",
            "Epoch 84/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9858 - val_loss: 0.0633 - val_acc: 0.9339\n",
            "Epoch 85/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0144 - acc: 0.9852 - val_loss: 0.0599 - val_acc: 0.9378\n",
            "Epoch 86/100\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0613 - val_acc: 0.9362\n",
            "Epoch 87/100\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0611 - val_acc: 0.9362\n",
            "Epoch 88/100\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0140 - acc: 0.9857 - val_loss: 0.0610 - val_acc: 0.9356\n",
            "Epoch 89/100\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0139 - acc: 0.9858 - val_loss: 0.0606 - val_acc: 0.9371\n",
            "Epoch 90/100\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0140 - acc: 0.9857 - val_loss: 0.0630 - val_acc: 0.9339\n",
            "Epoch 91/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0141 - acc: 0.9856 - val_loss: 0.0618 - val_acc: 0.9353\n",
            "Epoch 92/100\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0140 - acc: 0.9857 - val_loss: 0.0622 - val_acc: 0.9350\n",
            "Epoch 93/100\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0138 - acc: 0.9859 - val_loss: 0.0609 - val_acc: 0.9365\n",
            "Epoch 94/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0139 - acc: 0.9857 - val_loss: 0.0597 - val_acc: 0.9384\n",
            "Epoch 95/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9860 - val_loss: 0.0597 - val_acc: 0.9379\n",
            "Epoch 96/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9861 - val_loss: 0.0621 - val_acc: 0.9356\n",
            "Epoch 97/100\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0137 - acc: 0.9860 - val_loss: 0.0632 - val_acc: 0.9342\n",
            "Epoch 98/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0139 - acc: 0.9857 - val_loss: 0.0624 - val_acc: 0.9349\n",
            "Epoch 99/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9862 - val_loss: 0.0633 - val_acc: 0.9342\n",
            "Epoch 100/100\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9861 - val_loss: 0.0619 - val_acc: 0.9358\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/110\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0532 - acc: 0.9345 - val_loss: 0.0516 - val_acc: 0.9361\n",
            "Epoch 2/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0443 - acc: 0.9454 - val_loss: 0.0504 - val_acc: 0.9382\n",
            "Epoch 3/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0399 - acc: 0.9517 - val_loss: 0.0501 - val_acc: 0.9387\n",
            "Epoch 4/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0356 - acc: 0.9578 - val_loss: 0.0510 - val_acc: 0.9381\n",
            "Epoch 5/110\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0324 - acc: 0.9622 - val_loss: 0.0513 - val_acc: 0.9381\n",
            "Epoch 6/110\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0299 - acc: 0.9653 - val_loss: 0.0525 - val_acc: 0.9367\n",
            "Epoch 7/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0275 - acc: 0.9687 - val_loss: 0.0549 - val_acc: 0.9351\n",
            "Epoch 8/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0255 - acc: 0.9713 - val_loss: 0.0542 - val_acc: 0.9373\n",
            "Epoch 9/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0240 - acc: 0.9731 - val_loss: 0.0552 - val_acc: 0.9356\n",
            "Epoch 10/110\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0230 - acc: 0.9746 - val_loss: 0.0559 - val_acc: 0.9358\n",
            "Epoch 11/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0222 - acc: 0.9752 - val_loss: 0.0585 - val_acc: 0.9325\n",
            "Epoch 12/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0217 - acc: 0.9760 - val_loss: 0.0581 - val_acc: 0.9342\n",
            "Epoch 13/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0206 - acc: 0.9774 - val_loss: 0.0568 - val_acc: 0.9364\n",
            "Epoch 14/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0202 - acc: 0.9779 - val_loss: 0.0558 - val_acc: 0.9380\n",
            "Epoch 15/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0198 - acc: 0.9786 - val_loss: 0.0586 - val_acc: 0.9340\n",
            "Epoch 16/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0195 - acc: 0.9788 - val_loss: 0.0574 - val_acc: 0.9362\n",
            "Epoch 17/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0189 - acc: 0.9796 - val_loss: 0.0579 - val_acc: 0.9353\n",
            "Epoch 18/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0184 - acc: 0.9802 - val_loss: 0.0587 - val_acc: 0.9344\n",
            "Epoch 19/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0183 - acc: 0.9806 - val_loss: 0.0593 - val_acc: 0.9340\n",
            "Epoch 20/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0181 - acc: 0.9804 - val_loss: 0.0578 - val_acc: 0.9367\n",
            "Epoch 21/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0176 - acc: 0.9812 - val_loss: 0.0586 - val_acc: 0.9359\n",
            "Epoch 22/110\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0174 - acc: 0.9815 - val_loss: 0.0584 - val_acc: 0.9364\n",
            "Epoch 23/110\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0174 - acc: 0.9813 - val_loss: 0.0612 - val_acc: 0.9324\n",
            "Epoch 24/110\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0170 - acc: 0.9818 - val_loss: 0.0594 - val_acc: 0.9346\n",
            "Epoch 25/110\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0170 - acc: 0.9820 - val_loss: 0.0586 - val_acc: 0.9363\n",
            "Epoch 26/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0164 - acc: 0.9828 - val_loss: 0.0607 - val_acc: 0.9341\n",
            "Epoch 27/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0165 - acc: 0.9824 - val_loss: 0.0600 - val_acc: 0.9349\n",
            "Epoch 28/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0162 - acc: 0.9829 - val_loss: 0.0607 - val_acc: 0.9340\n",
            "Epoch 29/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0166 - acc: 0.9824 - val_loss: 0.0591 - val_acc: 0.9364\n",
            "Epoch 30/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0163 - acc: 0.9828 - val_loss: 0.0606 - val_acc: 0.9341\n",
            "Epoch 31/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0159 - acc: 0.9832 - val_loss: 0.0601 - val_acc: 0.9356\n",
            "Epoch 32/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0160 - acc: 0.9833 - val_loss: 0.0601 - val_acc: 0.9357\n",
            "Epoch 33/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9835 - val_loss: 0.0596 - val_acc: 0.9364\n",
            "Epoch 34/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0159 - acc: 0.9832 - val_loss: 0.0595 - val_acc: 0.9361\n",
            "Epoch 35/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0155 - acc: 0.9838 - val_loss: 0.0599 - val_acc: 0.9361\n",
            "Epoch 36/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0156 - acc: 0.9836 - val_loss: 0.0595 - val_acc: 0.9363\n",
            "Epoch 37/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0154 - acc: 0.9837 - val_loss: 0.0607 - val_acc: 0.9352\n",
            "Epoch 38/110\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0154 - acc: 0.9838 - val_loss: 0.0621 - val_acc: 0.9330\n",
            "Epoch 39/110\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0152 - acc: 0.9842 - val_loss: 0.0608 - val_acc: 0.9353\n",
            "Epoch 40/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9842 - val_loss: 0.0611 - val_acc: 0.9349\n",
            "Epoch 41/110\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0150 - acc: 0.9843 - val_loss: 0.0611 - val_acc: 0.9346\n",
            "Epoch 42/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9847 - val_loss: 0.0631 - val_acc: 0.9324\n",
            "Epoch 43/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9842 - val_loss: 0.0620 - val_acc: 0.9336\n",
            "Epoch 44/110\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0151 - acc: 0.9844 - val_loss: 0.0616 - val_acc: 0.9344\n",
            "Epoch 45/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9845 - val_loss: 0.0618 - val_acc: 0.9343\n",
            "Epoch 46/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0611 - val_acc: 0.9350\n",
            "Epoch 47/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9842 - val_loss: 0.0596 - val_acc: 0.9371\n",
            "Epoch 48/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9849 - val_loss: 0.0609 - val_acc: 0.9356\n",
            "Epoch 49/110\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0148 - acc: 0.9845 - val_loss: 0.0604 - val_acc: 0.9359\n",
            "Epoch 50/110\n",
            "102124/102124 [==============================] - 6s 57us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0627 - val_acc: 0.9330\n",
            "Epoch 51/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0146 - acc: 0.9849 - val_loss: 0.0617 - val_acc: 0.9345\n",
            "Epoch 52/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0617 - val_acc: 0.9344\n",
            "Epoch 53/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0144 - acc: 0.9851 - val_loss: 0.0624 - val_acc: 0.9333\n",
            "Epoch 54/110\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0144 - acc: 0.9851 - val_loss: 0.0606 - val_acc: 0.9362\n",
            "Epoch 55/110\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0145 - acc: 0.9849 - val_loss: 0.0640 - val_acc: 0.9323\n",
            "Epoch 56/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9844 - val_loss: 0.0610 - val_acc: 0.9360\n",
            "Epoch 57/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9854 - val_loss: 0.0607 - val_acc: 0.9358\n",
            "Epoch 58/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9853 - val_loss: 0.0611 - val_acc: 0.9355\n",
            "Epoch 59/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9853 - val_loss: 0.0608 - val_acc: 0.9358\n",
            "Epoch 60/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0139 - acc: 0.9857 - val_loss: 0.0624 - val_acc: 0.9339\n",
            "Epoch 61/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0141 - acc: 0.9854 - val_loss: 0.0628 - val_acc: 0.9334\n",
            "Epoch 62/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0144 - acc: 0.9850 - val_loss: 0.0608 - val_acc: 0.9359\n",
            "Epoch 63/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0144 - acc: 0.9850 - val_loss: 0.0633 - val_acc: 0.9334\n",
            "Epoch 64/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0141 - acc: 0.9853 - val_loss: 0.0630 - val_acc: 0.9332\n",
            "Epoch 65/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0140 - acc: 0.9856 - val_loss: 0.0629 - val_acc: 0.9337\n",
            "Epoch 66/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9856 - val_loss: 0.0643 - val_acc: 0.9322\n",
            "Epoch 67/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0140 - acc: 0.9855 - val_loss: 0.0618 - val_acc: 0.9350\n",
            "Epoch 68/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0139 - acc: 0.9857 - val_loss: 0.0618 - val_acc: 0.9352\n",
            "Epoch 69/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0138 - acc: 0.9858 - val_loss: 0.0632 - val_acc: 0.9331\n",
            "Epoch 70/110\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0141 - acc: 0.9855 - val_loss: 0.0651 - val_acc: 0.9313\n",
            "Epoch 71/110\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0139 - acc: 0.9856 - val_loss: 0.0632 - val_acc: 0.9332\n",
            "Epoch 72/110\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0139 - acc: 0.9857 - val_loss: 0.0608 - val_acc: 0.9364\n",
            "Epoch 73/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0139 - acc: 0.9855 - val_loss: 0.0632 - val_acc: 0.9337\n",
            "Epoch 74/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0141 - acc: 0.9855 - val_loss: 0.0634 - val_acc: 0.9333\n",
            "Epoch 75/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0139 - acc: 0.9857 - val_loss: 0.0622 - val_acc: 0.9349\n",
            "Epoch 76/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9856 - val_loss: 0.0625 - val_acc: 0.9346\n",
            "Epoch 77/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9861 - val_loss: 0.0643 - val_acc: 0.9324\n",
            "Epoch 78/110\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0139 - acc: 0.9856 - val_loss: 0.0628 - val_acc: 0.9345\n",
            "Epoch 79/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0137 - acc: 0.9859 - val_loss: 0.0624 - val_acc: 0.9347\n",
            "Epoch 80/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9860 - val_loss: 0.0633 - val_acc: 0.9339\n",
            "Epoch 81/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9860 - val_loss: 0.0635 - val_acc: 0.9338\n",
            "Epoch 82/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9864 - val_loss: 0.0623 - val_acc: 0.9345\n",
            "Epoch 83/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9865 - val_loss: 0.0637 - val_acc: 0.9333\n",
            "Epoch 84/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9863 - val_loss: 0.0629 - val_acc: 0.9348\n",
            "Epoch 85/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0138 - acc: 0.9858 - val_loss: 0.0636 - val_acc: 0.9335\n",
            "Epoch 86/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0633 - val_acc: 0.9343\n",
            "Epoch 87/110\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0135 - acc: 0.9862 - val_loss: 0.0626 - val_acc: 0.9348\n",
            "Epoch 88/110\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0136 - acc: 0.9861 - val_loss: 0.0622 - val_acc: 0.9349\n",
            "Epoch 89/110\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0136 - acc: 0.9861 - val_loss: 0.0628 - val_acc: 0.9339\n",
            "Epoch 90/110\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0134 - acc: 0.9863 - val_loss: 0.0638 - val_acc: 0.9337\n",
            "Epoch 91/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0622 - val_acc: 0.9357\n",
            "Epoch 92/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9861 - val_loss: 0.0641 - val_acc: 0.9328\n",
            "Epoch 93/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0139 - acc: 0.9857 - val_loss: 0.0642 - val_acc: 0.9330\n",
            "Epoch 94/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9864 - val_loss: 0.0619 - val_acc: 0.9357\n",
            "Epoch 95/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9864 - val_loss: 0.0637 - val_acc: 0.9335\n",
            "Epoch 96/110\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0138 - acc: 0.9857 - val_loss: 0.0628 - val_acc: 0.9348\n",
            "Epoch 97/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0131 - acc: 0.9867 - val_loss: 0.0633 - val_acc: 0.9343\n",
            "Epoch 98/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0132 - acc: 0.9866 - val_loss: 0.0634 - val_acc: 0.9335\n",
            "Epoch 99/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0135 - acc: 0.9861 - val_loss: 0.0632 - val_acc: 0.9343\n",
            "Epoch 100/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9861 - val_loss: 0.0611 - val_acc: 0.9370\n",
            "Epoch 101/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0132 - acc: 0.9865 - val_loss: 0.0617 - val_acc: 0.9361\n",
            "Epoch 102/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9864 - val_loss: 0.0644 - val_acc: 0.9326\n",
            "Epoch 103/110\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0638 - val_acc: 0.9329\n",
            "Epoch 104/110\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0611 - val_acc: 0.9367\n",
            "Epoch 105/110\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0615 - val_acc: 0.9362\n",
            "Epoch 106/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0639 - val_acc: 0.9337\n",
            "Epoch 107/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0624 - val_acc: 0.9352\n",
            "Epoch 108/110\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0132 - acc: 0.9865 - val_loss: 0.0622 - val_acc: 0.9358\n",
            "Epoch 109/110\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0133 - acc: 0.9864 - val_loss: 0.0626 - val_acc: 0.9349\n",
            "Epoch 110/110\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0642 - val_acc: 0.9334\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/120\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0550 - acc: 0.9322 - val_loss: 0.0525 - val_acc: 0.9347\n",
            "Epoch 2/120\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0444 - acc: 0.9450 - val_loss: 0.0498 - val_acc: 0.9384\n",
            "Epoch 3/120\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0397 - acc: 0.9512 - val_loss: 0.0497 - val_acc: 0.9379\n",
            "Epoch 4/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0358 - acc: 0.9564 - val_loss: 0.0505 - val_acc: 0.9375\n",
            "Epoch 5/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0324 - acc: 0.9615 - val_loss: 0.0515 - val_acc: 0.9364\n",
            "Epoch 6/120\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0295 - acc: 0.9656 - val_loss: 0.0518 - val_acc: 0.9364\n",
            "Epoch 7/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0272 - acc: 0.9688 - val_loss: 0.0537 - val_acc: 0.9347\n",
            "Epoch 8/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0254 - acc: 0.9712 - val_loss: 0.0543 - val_acc: 0.9355\n",
            "Epoch 9/120\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0237 - acc: 0.9732 - val_loss: 0.0543 - val_acc: 0.9360\n",
            "Epoch 10/120\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0226 - acc: 0.9750 - val_loss: 0.0561 - val_acc: 0.9338\n",
            "Epoch 11/120\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0216 - acc: 0.9758 - val_loss: 0.0558 - val_acc: 0.9355\n",
            "Epoch 12/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0211 - acc: 0.9766 - val_loss: 0.0591 - val_acc: 0.9313\n",
            "Epoch 13/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0201 - acc: 0.9781 - val_loss: 0.0559 - val_acc: 0.9363\n",
            "Epoch 14/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0198 - acc: 0.9782 - val_loss: 0.0568 - val_acc: 0.9361\n",
            "Epoch 15/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0187 - acc: 0.9797 - val_loss: 0.0599 - val_acc: 0.9311\n",
            "Epoch 16/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0185 - acc: 0.9799 - val_loss: 0.0591 - val_acc: 0.9333\n",
            "Epoch 17/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0187 - acc: 0.9797 - val_loss: 0.0595 - val_acc: 0.9333\n",
            "Epoch 18/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0177 - acc: 0.9810 - val_loss: 0.0602 - val_acc: 0.9328\n",
            "Epoch 19/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0177 - acc: 0.9808 - val_loss: 0.0573 - val_acc: 0.9367\n",
            "Epoch 20/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0172 - acc: 0.9815 - val_loss: 0.0583 - val_acc: 0.9360\n",
            "Epoch 21/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0173 - acc: 0.9813 - val_loss: 0.0585 - val_acc: 0.9360\n",
            "Epoch 22/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0171 - acc: 0.9817 - val_loss: 0.0592 - val_acc: 0.9347\n",
            "Epoch 23/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0165 - acc: 0.9823 - val_loss: 0.0585 - val_acc: 0.9369\n",
            "Epoch 24/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0165 - acc: 0.9823 - val_loss: 0.0610 - val_acc: 0.9334\n",
            "Epoch 25/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0161 - acc: 0.9829 - val_loss: 0.0595 - val_acc: 0.9350\n",
            "Epoch 26/120\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0166 - acc: 0.9823 - val_loss: 0.0600 - val_acc: 0.9350\n",
            "Epoch 27/120\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0163 - acc: 0.9825 - val_loss: 0.0590 - val_acc: 0.9364\n",
            "Epoch 28/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0160 - acc: 0.9830 - val_loss: 0.0616 - val_acc: 0.9335\n",
            "Epoch 29/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0161 - acc: 0.9830 - val_loss: 0.0591 - val_acc: 0.9365\n",
            "Epoch 30/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9832 - val_loss: 0.0620 - val_acc: 0.9323\n",
            "Epoch 31/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0153 - acc: 0.9841 - val_loss: 0.0627 - val_acc: 0.9316\n",
            "Epoch 32/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0157 - acc: 0.9832 - val_loss: 0.0664 - val_acc: 0.9275\n",
            "Epoch 33/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0152 - acc: 0.9840 - val_loss: 0.0614 - val_acc: 0.9338\n",
            "Epoch 34/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0154 - acc: 0.9838 - val_loss: 0.0618 - val_acc: 0.9327\n",
            "Epoch 35/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9843 - val_loss: 0.0609 - val_acc: 0.9347\n",
            "Epoch 36/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9841 - val_loss: 0.0617 - val_acc: 0.9340\n",
            "Epoch 37/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0150 - acc: 0.9841 - val_loss: 0.0619 - val_acc: 0.9336\n",
            "Epoch 38/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9841 - val_loss: 0.0599 - val_acc: 0.9361\n",
            "Epoch 39/120\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0146 - acc: 0.9847 - val_loss: 0.0602 - val_acc: 0.9364\n",
            "Epoch 40/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0147 - acc: 0.9847 - val_loss: 0.0643 - val_acc: 0.9311\n",
            "Epoch 41/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0149 - acc: 0.9845 - val_loss: 0.0614 - val_acc: 0.9344\n",
            "Epoch 42/120\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0152 - acc: 0.9839 - val_loss: 0.0605 - val_acc: 0.9363\n",
            "Epoch 43/120\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0141 - acc: 0.9855 - val_loss: 0.0598 - val_acc: 0.9371\n",
            "Epoch 44/120\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0144 - acc: 0.9849 - val_loss: 0.0604 - val_acc: 0.9365\n",
            "Epoch 45/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9849 - val_loss: 0.0618 - val_acc: 0.9348\n",
            "Epoch 46/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0678 - val_acc: 0.9278\n",
            "Epoch 47/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9851 - val_loss: 0.0619 - val_acc: 0.9343\n",
            "Epoch 48/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0144 - acc: 0.9850 - val_loss: 0.0605 - val_acc: 0.9366\n",
            "Epoch 49/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9847 - val_loss: 0.0643 - val_acc: 0.9313\n",
            "Epoch 50/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0628 - val_acc: 0.9335\n",
            "Epoch 51/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0141 - acc: 0.9854 - val_loss: 0.0603 - val_acc: 0.9364\n",
            "Epoch 52/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9851 - val_loss: 0.0623 - val_acc: 0.9341\n",
            "Epoch 53/120\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0143 - acc: 0.9851 - val_loss: 0.0632 - val_acc: 0.9329\n",
            "Epoch 54/120\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0137 - acc: 0.9857 - val_loss: 0.0602 - val_acc: 0.9367\n",
            "Epoch 55/120\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0139 - acc: 0.9855 - val_loss: 0.0618 - val_acc: 0.9353\n",
            "Epoch 56/120\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0617 - val_acc: 0.9352\n",
            "Epoch 57/120\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0138 - acc: 0.9857 - val_loss: 0.0618 - val_acc: 0.9347\n",
            "Epoch 58/120\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0138 - acc: 0.9857 - val_loss: 0.0620 - val_acc: 0.9351\n",
            "Epoch 59/120\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0618 - val_acc: 0.9351\n",
            "Epoch 60/120\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0625 - val_acc: 0.9346\n",
            "Epoch 61/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0626 - val_acc: 0.9338\n",
            "Epoch 62/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9858 - val_loss: 0.0653 - val_acc: 0.9313\n",
            "Epoch 63/120\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0629 - val_acc: 0.9333\n",
            "Epoch 64/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0631 - val_acc: 0.9335\n",
            "Epoch 65/120\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0655 - val_acc: 0.9310\n",
            "Epoch 66/120\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0651 - val_acc: 0.9311\n",
            "Epoch 67/120\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0132 - acc: 0.9863 - val_loss: 0.0617 - val_acc: 0.9355\n",
            "Epoch 68/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0635 - val_acc: 0.9333\n",
            "Epoch 69/120\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0619 - val_acc: 0.9355\n",
            "Epoch 70/120\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0627 - val_acc: 0.9346\n",
            "Epoch 71/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0654 - val_acc: 0.9308\n",
            "Epoch 72/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0629 - val_acc: 0.9343\n",
            "Epoch 73/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0661 - val_acc: 0.9303\n",
            "Epoch 74/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0135 - acc: 0.9859 - val_loss: 0.0645 - val_acc: 0.9319\n",
            "Epoch 75/120\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0137 - acc: 0.9859 - val_loss: 0.0639 - val_acc: 0.9330\n",
            "Epoch 76/120\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0134 - acc: 0.9863 - val_loss: 0.0663 - val_acc: 0.9307\n",
            "Epoch 77/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0132 - acc: 0.9865 - val_loss: 0.0641 - val_acc: 0.9329\n",
            "Epoch 78/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9864 - val_loss: 0.0649 - val_acc: 0.9321\n",
            "Epoch 79/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0677 - val_acc: 0.9290\n",
            "Epoch 80/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0654 - val_acc: 0.9320\n",
            "Epoch 81/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0636 - val_acc: 0.9336\n",
            "Epoch 82/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9862 - val_loss: 0.0641 - val_acc: 0.9330\n",
            "Epoch 83/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0131 - acc: 0.9863 - val_loss: 0.0639 - val_acc: 0.9331\n",
            "Epoch 84/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0638 - val_acc: 0.9334\n",
            "Epoch 85/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9861 - val_loss: 0.0657 - val_acc: 0.9312\n",
            "Epoch 86/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0132 - acc: 0.9864 - val_loss: 0.0630 - val_acc: 0.9344\n",
            "Epoch 87/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0131 - acc: 0.9865 - val_loss: 0.0618 - val_acc: 0.9358\n",
            "Epoch 88/120\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0131 - acc: 0.9865 - val_loss: 0.0624 - val_acc: 0.9353\n",
            "Epoch 89/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0129 - acc: 0.9867 - val_loss: 0.0629 - val_acc: 0.9347\n",
            "Epoch 90/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0656 - val_acc: 0.9313\n",
            "Epoch 91/120\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0133 - acc: 0.9862 - val_loss: 0.0678 - val_acc: 0.9291\n",
            "Epoch 92/120\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0645 - val_acc: 0.9333\n",
            "Epoch 93/120\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0128 - acc: 0.9867 - val_loss: 0.0646 - val_acc: 0.9328\n",
            "Epoch 94/120\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0631 - val_acc: 0.9342\n",
            "Epoch 95/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0632 - val_acc: 0.9343\n",
            "Epoch 96/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0128 - acc: 0.9868 - val_loss: 0.0649 - val_acc: 0.9325\n",
            "Epoch 97/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0636 - val_acc: 0.9341\n",
            "Epoch 98/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0126 - acc: 0.9871 - val_loss: 0.0638 - val_acc: 0.9341\n",
            "Epoch 99/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0645 - val_acc: 0.9332\n",
            "Epoch 100/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0126 - acc: 0.9870 - val_loss: 0.0662 - val_acc: 0.9311\n",
            "Epoch 101/120\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0127 - acc: 0.9871 - val_loss: 0.0641 - val_acc: 0.9335\n",
            "Epoch 102/120\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0642 - val_acc: 0.9336\n",
            "Epoch 103/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0650 - val_acc: 0.9323\n",
            "Epoch 104/120\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0652 - val_acc: 0.9329\n",
            "Epoch 105/120\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0130 - acc: 0.9866 - val_loss: 0.0648 - val_acc: 0.9325\n",
            "Epoch 106/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0657 - val_acc: 0.9317\n",
            "Epoch 107/120\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0128 - acc: 0.9868 - val_loss: 0.0641 - val_acc: 0.9330\n",
            "Epoch 108/120\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0627 - val_acc: 0.9346\n",
            "Epoch 109/120\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0665 - val_acc: 0.9306\n",
            "Epoch 110/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0132 - acc: 0.9863 - val_loss: 0.0662 - val_acc: 0.9307\n",
            "Epoch 111/120\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0646 - val_acc: 0.9330\n",
            "Epoch 112/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0653 - val_acc: 0.9324\n",
            "Epoch 113/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9873 - val_loss: 0.0660 - val_acc: 0.9312\n",
            "Epoch 114/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0641 - val_acc: 0.9334\n",
            "Epoch 115/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0647 - val_acc: 0.9333\n",
            "Epoch 116/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0122 - acc: 0.9876 - val_loss: 0.0651 - val_acc: 0.9330\n",
            "Epoch 117/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0124 - acc: 0.9874 - val_loss: 0.0636 - val_acc: 0.9342\n",
            "Epoch 118/120\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0122 - acc: 0.9876 - val_loss: 0.0665 - val_acc: 0.9311\n",
            "Epoch 119/120\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0125 - acc: 0.9871 - val_loss: 0.0641 - val_acc: 0.9339\n",
            "Epoch 120/120\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0124 - acc: 0.9872 - val_loss: 0.0641 - val_acc: 0.9335\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/130\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0541 - acc: 0.9331 - val_loss: 0.0516 - val_acc: 0.9366\n",
            "Epoch 2/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0444 - acc: 0.9448 - val_loss: 0.0508 - val_acc: 0.9360\n",
            "Epoch 3/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0398 - acc: 0.9514 - val_loss: 0.0498 - val_acc: 0.9381\n",
            "Epoch 4/130\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0356 - acc: 0.9575 - val_loss: 0.0516 - val_acc: 0.9353\n",
            "Epoch 5/130\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0322 - acc: 0.9621 - val_loss: 0.0523 - val_acc: 0.9369\n",
            "Epoch 6/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0294 - acc: 0.9662 - val_loss: 0.0524 - val_acc: 0.9380\n",
            "Epoch 7/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0270 - acc: 0.9693 - val_loss: 0.0562 - val_acc: 0.9317\n",
            "Epoch 8/130\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0254 - acc: 0.9712 - val_loss: 0.0537 - val_acc: 0.9369\n",
            "Epoch 9/130\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0237 - acc: 0.9738 - val_loss: 0.0554 - val_acc: 0.9360\n",
            "Epoch 10/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0229 - acc: 0.9744 - val_loss: 0.0553 - val_acc: 0.9362\n",
            "Epoch 11/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0219 - acc: 0.9757 - val_loss: 0.0573 - val_acc: 0.9338\n",
            "Epoch 12/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0209 - acc: 0.9770 - val_loss: 0.0556 - val_acc: 0.9374\n",
            "Epoch 13/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0199 - acc: 0.9784 - val_loss: 0.0579 - val_acc: 0.9338\n",
            "Epoch 14/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0199 - acc: 0.9782 - val_loss: 0.0572 - val_acc: 0.9355\n",
            "Epoch 15/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0191 - acc: 0.9792 - val_loss: 0.0569 - val_acc: 0.9361\n",
            "Epoch 16/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0186 - acc: 0.9799 - val_loss: 0.0572 - val_acc: 0.9358\n",
            "Epoch 17/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0183 - acc: 0.9801 - val_loss: 0.0581 - val_acc: 0.9356\n",
            "Epoch 18/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0178 - acc: 0.9808 - val_loss: 0.0573 - val_acc: 0.9371\n",
            "Epoch 19/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0176 - acc: 0.9811 - val_loss: 0.0584 - val_acc: 0.9347\n",
            "Epoch 20/130\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0169 - acc: 0.9819 - val_loss: 0.0600 - val_acc: 0.9334\n",
            "Epoch 21/130\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0172 - acc: 0.9815 - val_loss: 0.0586 - val_acc: 0.9357\n",
            "Epoch 22/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0169 - acc: 0.9819 - val_loss: 0.0600 - val_acc: 0.9338\n",
            "Epoch 23/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0163 - acc: 0.9826 - val_loss: 0.0594 - val_acc: 0.9343\n",
            "Epoch 24/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0162 - acc: 0.9829 - val_loss: 0.0603 - val_acc: 0.9346\n",
            "Epoch 25/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0160 - acc: 0.9831 - val_loss: 0.0615 - val_acc: 0.9326\n",
            "Epoch 26/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0159 - acc: 0.9830 - val_loss: 0.0591 - val_acc: 0.9351\n",
            "Epoch 27/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0162 - acc: 0.9826 - val_loss: 0.0584 - val_acc: 0.9361\n",
            "Epoch 28/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0160 - acc: 0.9829 - val_loss: 0.0581 - val_acc: 0.9376\n",
            "Epoch 29/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0153 - acc: 0.9838 - val_loss: 0.0600 - val_acc: 0.9346\n",
            "Epoch 30/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0153 - acc: 0.9839 - val_loss: 0.0627 - val_acc: 0.9320\n",
            "Epoch 31/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0158 - acc: 0.9834 - val_loss: 0.0605 - val_acc: 0.9350\n",
            "Epoch 32/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0156 - acc: 0.9834 - val_loss: 0.0619 - val_acc: 0.9326\n",
            "Epoch 33/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0152 - acc: 0.9840 - val_loss: 0.0594 - val_acc: 0.9359\n",
            "Epoch 34/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0155 - acc: 0.9836 - val_loss: 0.0597 - val_acc: 0.9364\n",
            "Epoch 35/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0152 - acc: 0.9840 - val_loss: 0.0588 - val_acc: 0.9369\n",
            "Epoch 36/130\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0148 - acc: 0.9844 - val_loss: 0.0593 - val_acc: 0.9370\n",
            "Epoch 37/130\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0148 - acc: 0.9846 - val_loss: 0.0598 - val_acc: 0.9361\n",
            "Epoch 38/130\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0150 - acc: 0.9842 - val_loss: 0.0607 - val_acc: 0.9343\n",
            "Epoch 39/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9846 - val_loss: 0.0603 - val_acc: 0.9352\n",
            "Epoch 40/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9844 - val_loss: 0.0631 - val_acc: 0.9317\n",
            "Epoch 41/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0148 - acc: 0.9845 - val_loss: 0.0598 - val_acc: 0.9368\n",
            "Epoch 42/130\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0145 - acc: 0.9849 - val_loss: 0.0635 - val_acc: 0.9318\n",
            "Epoch 43/130\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0145 - acc: 0.9849 - val_loss: 0.0613 - val_acc: 0.9349\n",
            "Epoch 44/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9852 - val_loss: 0.0609 - val_acc: 0.9347\n",
            "Epoch 45/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0143 - acc: 0.9850 - val_loss: 0.0620 - val_acc: 0.9335\n",
            "Epoch 46/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0143 - acc: 0.9851 - val_loss: 0.0597 - val_acc: 0.9361\n",
            "Epoch 47/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9855 - val_loss: 0.0622 - val_acc: 0.9329\n",
            "Epoch 48/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0142 - acc: 0.9852 - val_loss: 0.0611 - val_acc: 0.9353\n",
            "Epoch 49/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0138 - acc: 0.9857 - val_loss: 0.0592 - val_acc: 0.9380\n",
            "Epoch 50/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0140 - acc: 0.9854 - val_loss: 0.0592 - val_acc: 0.9374\n",
            "Epoch 51/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0141 - acc: 0.9854 - val_loss: 0.0614 - val_acc: 0.9346\n",
            "Epoch 52/130\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0142 - acc: 0.9852 - val_loss: 0.0593 - val_acc: 0.9370\n",
            "Epoch 53/130\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0141 - acc: 0.9852 - val_loss: 0.0606 - val_acc: 0.9360\n",
            "Epoch 54/130\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0616 - val_acc: 0.9344\n",
            "Epoch 55/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0137 - acc: 0.9857 - val_loss: 0.0597 - val_acc: 0.9375\n",
            "Epoch 56/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9858 - val_loss: 0.0597 - val_acc: 0.9366\n",
            "Epoch 57/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0602 - val_acc: 0.9364\n",
            "Epoch 58/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0138 - acc: 0.9856 - val_loss: 0.0608 - val_acc: 0.9357\n",
            "Epoch 59/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9860 - val_loss: 0.0632 - val_acc: 0.9328\n",
            "Epoch 60/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0135 - acc: 0.9860 - val_loss: 0.0603 - val_acc: 0.9360\n",
            "Epoch 61/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9860 - val_loss: 0.0628 - val_acc: 0.9333\n",
            "Epoch 62/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0631 - val_acc: 0.9328\n",
            "Epoch 63/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0617 - val_acc: 0.9351\n",
            "Epoch 64/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0135 - acc: 0.9861 - val_loss: 0.0613 - val_acc: 0.9353\n",
            "Epoch 65/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0135 - acc: 0.9862 - val_loss: 0.0613 - val_acc: 0.9358\n",
            "Epoch 66/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0135 - acc: 0.9861 - val_loss: 0.0612 - val_acc: 0.9356\n",
            "Epoch 67/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0610 - val_acc: 0.9359\n",
            "Epoch 68/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0614 - val_acc: 0.9352\n",
            "Epoch 69/130\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0618 - val_acc: 0.9355\n",
            "Epoch 70/130\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0632 - val_acc: 0.9337\n",
            "Epoch 71/130\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0130 - acc: 0.9868 - val_loss: 0.0608 - val_acc: 0.9362\n",
            "Epoch 72/130\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0134 - acc: 0.9861 - val_loss: 0.0617 - val_acc: 0.9352\n",
            "Epoch 73/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0625 - val_acc: 0.9346\n",
            "Epoch 74/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0624 - val_acc: 0.9341\n",
            "Epoch 75/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0134 - acc: 0.9863 - val_loss: 0.0620 - val_acc: 0.9348\n",
            "Epoch 76/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0132 - acc: 0.9865 - val_loss: 0.0611 - val_acc: 0.9358\n",
            "Epoch 77/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0133 - acc: 0.9862 - val_loss: 0.0623 - val_acc: 0.9347\n",
            "Epoch 78/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0132 - acc: 0.9864 - val_loss: 0.0630 - val_acc: 0.9338\n",
            "Epoch 79/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0619 - val_acc: 0.9351\n",
            "Epoch 80/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0599 - val_acc: 0.9375\n",
            "Epoch 81/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0130 - acc: 0.9866 - val_loss: 0.0603 - val_acc: 0.9371\n",
            "Epoch 82/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0612 - val_acc: 0.9358\n",
            "Epoch 83/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0131 - acc: 0.9865 - val_loss: 0.0605 - val_acc: 0.9368\n",
            "Epoch 84/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0606 - val_acc: 0.9374\n",
            "Epoch 85/130\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0130 - acc: 0.9865 - val_loss: 0.0612 - val_acc: 0.9358\n",
            "Epoch 86/130\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0613 - val_acc: 0.9359\n",
            "Epoch 87/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0132 - acc: 0.9865 - val_loss: 0.0614 - val_acc: 0.9357\n",
            "Epoch 88/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0132 - acc: 0.9864 - val_loss: 0.0607 - val_acc: 0.9369\n",
            "Epoch 89/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0129 - acc: 0.9869 - val_loss: 0.0636 - val_acc: 0.9333\n",
            "Epoch 90/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9871 - val_loss: 0.0607 - val_acc: 0.9364\n",
            "Epoch 91/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0129 - acc: 0.9867 - val_loss: 0.0619 - val_acc: 0.9349\n",
            "Epoch 92/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0129 - acc: 0.9869 - val_loss: 0.0610 - val_acc: 0.9364\n",
            "Epoch 93/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0126 - acc: 0.9870 - val_loss: 0.0617 - val_acc: 0.9355\n",
            "Epoch 94/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0621 - val_acc: 0.9352\n",
            "Epoch 95/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0126 - acc: 0.9871 - val_loss: 0.0619 - val_acc: 0.9358\n",
            "Epoch 96/130\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0629 - val_acc: 0.9343\n",
            "Epoch 97/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0129 - acc: 0.9867 - val_loss: 0.0624 - val_acc: 0.9351\n",
            "Epoch 98/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0608 - val_acc: 0.9372\n",
            "Epoch 99/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0603 - val_acc: 0.9376\n",
            "Epoch 100/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0128 - acc: 0.9870 - val_loss: 0.0625 - val_acc: 0.9349\n",
            "Epoch 101/130\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0124 - acc: 0.9874 - val_loss: 0.0614 - val_acc: 0.9362\n",
            "Epoch 102/130\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0623 - val_acc: 0.9354\n",
            "Epoch 103/130\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0625 - val_acc: 0.9347\n",
            "Epoch 104/130\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0129 - acc: 0.9867 - val_loss: 0.0599 - val_acc: 0.9383\n",
            "Epoch 105/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0612 - val_acc: 0.9366\n",
            "Epoch 106/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0128 - acc: 0.9868 - val_loss: 0.0609 - val_acc: 0.9367\n",
            "Epoch 107/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0127 - acc: 0.9871 - val_loss: 0.0610 - val_acc: 0.9369\n",
            "Epoch 108/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0618 - val_acc: 0.9355\n",
            "Epoch 109/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0629 - val_acc: 0.9347\n",
            "Epoch 110/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0126 - acc: 0.9871 - val_loss: 0.0649 - val_acc: 0.9322\n",
            "Epoch 111/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0129 - acc: 0.9869 - val_loss: 0.0641 - val_acc: 0.9331\n",
            "Epoch 112/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0128 - acc: 0.9868 - val_loss: 0.0633 - val_acc: 0.9344\n",
            "Epoch 113/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0125 - acc: 0.9873 - val_loss: 0.0618 - val_acc: 0.9355\n",
            "Epoch 114/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0651 - val_acc: 0.9321\n",
            "Epoch 115/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0623 - val_acc: 0.9355\n",
            "Epoch 116/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0639 - val_acc: 0.9331\n",
            "Epoch 117/130\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0128 - acc: 0.9870 - val_loss: 0.0635 - val_acc: 0.9344\n",
            "Epoch 118/130\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0621 - val_acc: 0.9351\n",
            "Epoch 119/130\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0126 - acc: 0.9871 - val_loss: 0.0614 - val_acc: 0.9363\n",
            "Epoch 120/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0599 - val_acc: 0.9382\n",
            "Epoch 121/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0122 - acc: 0.9877 - val_loss: 0.0628 - val_acc: 0.9347\n",
            "Epoch 122/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0615 - val_acc: 0.9364\n",
            "Epoch 123/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0605 - val_acc: 0.9374\n",
            "Epoch 124/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0125 - acc: 0.9873 - val_loss: 0.0612 - val_acc: 0.9365\n",
            "Epoch 125/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0623 - val_acc: 0.9355\n",
            "Epoch 126/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0628 - val_acc: 0.9350\n",
            "Epoch 127/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0125 - acc: 0.9873 - val_loss: 0.0620 - val_acc: 0.9362\n",
            "Epoch 128/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9873 - val_loss: 0.0616 - val_acc: 0.9364\n",
            "Epoch 129/130\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0622 - val_acc: 0.9355\n",
            "Epoch 130/130\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0629 - val_acc: 0.9349\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/140\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0546 - acc: 0.9317 - val_loss: 0.0514 - val_acc: 0.9363\n",
            "Epoch 2/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0448 - acc: 0.9443 - val_loss: 0.0494 - val_acc: 0.9382\n",
            "Epoch 3/140\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0401 - acc: 0.9514 - val_loss: 0.0497 - val_acc: 0.9384\n",
            "Epoch 4/140\n",
            "102124/102124 [==============================] - 6s 57us/step - loss: 0.0362 - acc: 0.9564 - val_loss: 0.0506 - val_acc: 0.9388\n",
            "Epoch 5/140\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0329 - acc: 0.9607 - val_loss: 0.0506 - val_acc: 0.9400\n",
            "Epoch 6/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0299 - acc: 0.9649 - val_loss: 0.0526 - val_acc: 0.9367\n",
            "Epoch 7/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0275 - acc: 0.9681 - val_loss: 0.0532 - val_acc: 0.9376\n",
            "Epoch 8/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0256 - acc: 0.9709 - val_loss: 0.0539 - val_acc: 0.9360\n",
            "Epoch 9/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0241 - acc: 0.9732 - val_loss: 0.0553 - val_acc: 0.9343\n",
            "Epoch 10/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0231 - acc: 0.9742 - val_loss: 0.0559 - val_acc: 0.9355\n",
            "Epoch 11/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0221 - acc: 0.9756 - val_loss: 0.0554 - val_acc: 0.9358\n",
            "Epoch 12/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0215 - acc: 0.9762 - val_loss: 0.0555 - val_acc: 0.9366\n",
            "Epoch 13/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0205 - acc: 0.9775 - val_loss: 0.0561 - val_acc: 0.9363\n",
            "Epoch 14/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0197 - acc: 0.9784 - val_loss: 0.0569 - val_acc: 0.9356\n",
            "Epoch 15/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0193 - acc: 0.9789 - val_loss: 0.0602 - val_acc: 0.9315\n",
            "Epoch 16/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0188 - acc: 0.9796 - val_loss: 0.0570 - val_acc: 0.9369\n",
            "Epoch 17/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0187 - acc: 0.9797 - val_loss: 0.0568 - val_acc: 0.9378\n",
            "Epoch 18/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0176 - acc: 0.9809 - val_loss: 0.0584 - val_acc: 0.9348\n",
            "Epoch 19/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0180 - acc: 0.9802 - val_loss: 0.0579 - val_acc: 0.9357\n",
            "Epoch 20/140\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0175 - acc: 0.9811 - val_loss: 0.0588 - val_acc: 0.9344\n",
            "Epoch 21/140\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0174 - acc: 0.9813 - val_loss: 0.0588 - val_acc: 0.9353\n",
            "Epoch 22/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0168 - acc: 0.9821 - val_loss: 0.0585 - val_acc: 0.9355\n",
            "Epoch 23/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0169 - acc: 0.9818 - val_loss: 0.0587 - val_acc: 0.9357\n",
            "Epoch 24/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0167 - acc: 0.9820 - val_loss: 0.0589 - val_acc: 0.9361\n",
            "Epoch 25/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0162 - acc: 0.9828 - val_loss: 0.0582 - val_acc: 0.9362\n",
            "Epoch 26/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0163 - acc: 0.9825 - val_loss: 0.0590 - val_acc: 0.9353\n",
            "Epoch 27/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0163 - acc: 0.9824 - val_loss: 0.0598 - val_acc: 0.9340\n",
            "Epoch 28/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0162 - acc: 0.9828 - val_loss: 0.0581 - val_acc: 0.9369\n",
            "Epoch 29/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0162 - acc: 0.9828 - val_loss: 0.0622 - val_acc: 0.9327\n",
            "Epoch 30/140\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0160 - acc: 0.9829 - val_loss: 0.0618 - val_acc: 0.9333\n",
            "Epoch 31/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0158 - acc: 0.9832 - val_loss: 0.0589 - val_acc: 0.9365\n",
            "Epoch 32/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0160 - acc: 0.9828 - val_loss: 0.0615 - val_acc: 0.9332\n",
            "Epoch 33/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0151 - acc: 0.9842 - val_loss: 0.0594 - val_acc: 0.9361\n",
            "Epoch 34/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0155 - acc: 0.9835 - val_loss: 0.0600 - val_acc: 0.9358\n",
            "Epoch 35/140\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0154 - acc: 0.9839 - val_loss: 0.0602 - val_acc: 0.9349\n",
            "Epoch 36/140\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0151 - acc: 0.9840 - val_loss: 0.0600 - val_acc: 0.9356\n",
            "Epoch 37/140\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0146 - acc: 0.9847 - val_loss: 0.0585 - val_acc: 0.9377\n",
            "Epoch 38/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9841 - val_loss: 0.0598 - val_acc: 0.9356\n",
            "Epoch 39/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0153 - acc: 0.9840 - val_loss: 0.0605 - val_acc: 0.9355\n",
            "Epoch 40/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0151 - acc: 0.9841 - val_loss: 0.0602 - val_acc: 0.9355\n",
            "Epoch 41/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0146 - acc: 0.9847 - val_loss: 0.0609 - val_acc: 0.9355\n",
            "Epoch 42/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0145 - acc: 0.9850 - val_loss: 0.0594 - val_acc: 0.9377\n",
            "Epoch 43/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0150 - acc: 0.9843 - val_loss: 0.0602 - val_acc: 0.9359\n",
            "Epoch 44/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0603 - val_acc: 0.9364\n",
            "Epoch 45/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0145 - acc: 0.9848 - val_loss: 0.0616 - val_acc: 0.9345\n",
            "Epoch 46/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9848 - val_loss: 0.0637 - val_acc: 0.9316\n",
            "Epoch 47/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9852 - val_loss: 0.0589 - val_acc: 0.9379\n",
            "Epoch 48/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0145 - acc: 0.9848 - val_loss: 0.0593 - val_acc: 0.9378\n",
            "Epoch 49/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9856 - val_loss: 0.0614 - val_acc: 0.9347\n",
            "Epoch 50/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0141 - acc: 0.9854 - val_loss: 0.0616 - val_acc: 0.9348\n",
            "Epoch 51/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0140 - acc: 0.9855 - val_loss: 0.0620 - val_acc: 0.9342\n",
            "Epoch 52/140\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0144 - acc: 0.9851 - val_loss: 0.0615 - val_acc: 0.9345\n",
            "Epoch 53/140\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0144 - acc: 0.9850 - val_loss: 0.0595 - val_acc: 0.9372\n",
            "Epoch 54/140\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0143 - acc: 0.9851 - val_loss: 0.0580 - val_acc: 0.9391\n",
            "Epoch 55/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0144 - acc: 0.9850 - val_loss: 0.0606 - val_acc: 0.9356\n",
            "Epoch 56/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0138 - acc: 0.9856 - val_loss: 0.0599 - val_acc: 0.9371\n",
            "Epoch 57/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0141 - acc: 0.9853 - val_loss: 0.0618 - val_acc: 0.9343\n",
            "Epoch 58/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0140 - acc: 0.9853 - val_loss: 0.0617 - val_acc: 0.9348\n",
            "Epoch 59/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0615 - val_acc: 0.9348\n",
            "Epoch 60/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0612 - val_acc: 0.9353\n",
            "Epoch 61/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0139 - acc: 0.9855 - val_loss: 0.0624 - val_acc: 0.9341\n",
            "Epoch 62/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0139 - acc: 0.9855 - val_loss: 0.0621 - val_acc: 0.9343\n",
            "Epoch 63/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0625 - val_acc: 0.9342\n",
            "Epoch 64/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0135 - acc: 0.9862 - val_loss: 0.0612 - val_acc: 0.9355\n",
            "Epoch 65/140\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0137 - acc: 0.9857 - val_loss: 0.0644 - val_acc: 0.9315\n",
            "Epoch 66/140\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0137 - acc: 0.9859 - val_loss: 0.0593 - val_acc: 0.9382\n",
            "Epoch 67/140\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0597 - val_acc: 0.9375\n",
            "Epoch 68/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0139 - acc: 0.9854 - val_loss: 0.0598 - val_acc: 0.9375\n",
            "Epoch 69/140\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0136 - acc: 0.9858 - val_loss: 0.0591 - val_acc: 0.9384\n",
            "Epoch 70/140\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0135 - acc: 0.9860 - val_loss: 0.0617 - val_acc: 0.9351\n",
            "Epoch 71/140\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0135 - acc: 0.9860 - val_loss: 0.0625 - val_acc: 0.9336\n",
            "Epoch 72/140\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0132 - acc: 0.9865 - val_loss: 0.0615 - val_acc: 0.9356\n",
            "Epoch 73/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0137 - acc: 0.9856 - val_loss: 0.0612 - val_acc: 0.9360\n",
            "Epoch 74/140\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0614 - val_acc: 0.9356\n",
            "Epoch 75/140\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0607 - val_acc: 0.9367\n",
            "Epoch 76/140\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0132 - acc: 0.9863 - val_loss: 0.0605 - val_acc: 0.9373\n",
            "Epoch 77/140\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0135 - acc: 0.9860 - val_loss: 0.0612 - val_acc: 0.9359\n",
            "Epoch 78/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0131 - acc: 0.9864 - val_loss: 0.0609 - val_acc: 0.9363\n",
            "Epoch 79/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0604 - val_acc: 0.9369\n",
            "Epoch 80/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0132 - acc: 0.9863 - val_loss: 0.0612 - val_acc: 0.9356\n",
            "Epoch 81/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0131 - acc: 0.9865 - val_loss: 0.0611 - val_acc: 0.9361\n",
            "Epoch 82/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0599 - val_acc: 0.9372\n",
            "Epoch 83/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0130 - acc: 0.9865 - val_loss: 0.0638 - val_acc: 0.9324\n",
            "Epoch 84/140\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0609 - val_acc: 0.9364\n",
            "Epoch 85/140\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0131 - acc: 0.9865 - val_loss: 0.0619 - val_acc: 0.9352\n",
            "Epoch 86/140\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0134 - acc: 0.9861 - val_loss: 0.0613 - val_acc: 0.9361\n",
            "Epoch 87/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0612 - val_acc: 0.9364\n",
            "Epoch 88/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0130 - acc: 0.9866 - val_loss: 0.0612 - val_acc: 0.9363\n",
            "Epoch 89/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0618 - val_acc: 0.9357\n",
            "Epoch 90/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0130 - acc: 0.9866 - val_loss: 0.0607 - val_acc: 0.9371\n",
            "Epoch 91/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0130 - acc: 0.9866 - val_loss: 0.0621 - val_acc: 0.9352\n",
            "Epoch 92/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0619 - val_acc: 0.9355\n",
            "Epoch 93/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0126 - acc: 0.9871 - val_loss: 0.0607 - val_acc: 0.9367\n",
            "Epoch 94/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0628 - val_acc: 0.9340\n",
            "Epoch 95/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0619 - val_acc: 0.9352\n",
            "Epoch 96/140\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0613 - val_acc: 0.9362\n",
            "Epoch 97/140\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0126 - acc: 0.9871 - val_loss: 0.0627 - val_acc: 0.9345\n",
            "Epoch 98/140\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0129 - acc: 0.9867 - val_loss: 0.0615 - val_acc: 0.9362\n",
            "Epoch 99/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0611 - val_acc: 0.9363\n",
            "Epoch 100/140\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0128 - acc: 0.9868 - val_loss: 0.0602 - val_acc: 0.9376\n",
            "Epoch 101/140\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0644 - val_acc: 0.9328\n",
            "Epoch 102/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0128 - acc: 0.9870 - val_loss: 0.0628 - val_acc: 0.9343\n",
            "Epoch 103/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0612 - val_acc: 0.9371\n",
            "Epoch 104/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0642 - val_acc: 0.9328\n",
            "Epoch 105/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0129 - acc: 0.9867 - val_loss: 0.0622 - val_acc: 0.9351\n",
            "Epoch 106/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0617 - val_acc: 0.9359\n",
            "Epoch 107/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0619 - val_acc: 0.9356\n",
            "Epoch 108/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0632 - val_acc: 0.9338\n",
            "Epoch 109/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0620 - val_acc: 0.9355\n",
            "Epoch 110/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0126 - acc: 0.9871 - val_loss: 0.0626 - val_acc: 0.9345\n",
            "Epoch 111/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0620 - val_acc: 0.9358\n",
            "Epoch 112/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0626 - val_acc: 0.9346\n",
            "Epoch 113/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0627 - val_acc: 0.9344\n",
            "Epoch 114/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0617 - val_acc: 0.9360\n",
            "Epoch 115/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0124 - acc: 0.9875 - val_loss: 0.0621 - val_acc: 0.9355\n",
            "Epoch 116/140\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0647 - val_acc: 0.9327\n",
            "Epoch 117/140\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0620 - val_acc: 0.9356\n",
            "Epoch 118/140\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0121 - acc: 0.9878 - val_loss: 0.0633 - val_acc: 0.9336\n",
            "Epoch 119/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0630 - val_acc: 0.9349\n",
            "Epoch 120/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0631 - val_acc: 0.9343\n",
            "Epoch 121/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0630 - val_acc: 0.9347\n",
            "Epoch 122/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0124 - acc: 0.9872 - val_loss: 0.0632 - val_acc: 0.9344\n",
            "Epoch 123/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0122 - acc: 0.9876 - val_loss: 0.0612 - val_acc: 0.9367\n",
            "Epoch 124/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0645 - val_acc: 0.9333\n",
            "Epoch 125/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0121 - acc: 0.9876 - val_loss: 0.0610 - val_acc: 0.9369\n",
            "Epoch 126/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0124 - acc: 0.9874 - val_loss: 0.0602 - val_acc: 0.9377\n",
            "Epoch 127/140\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0642 - val_acc: 0.9337\n",
            "Epoch 128/140\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0120 - acc: 0.9878 - val_loss: 0.0629 - val_acc: 0.9347\n",
            "Epoch 129/140\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0122 - acc: 0.9876 - val_loss: 0.0639 - val_acc: 0.9336\n",
            "Epoch 130/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0606 - val_acc: 0.9378\n",
            "Epoch 131/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0623 - val_acc: 0.9351\n",
            "Epoch 132/140\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0632 - val_acc: 0.9340\n",
            "Epoch 133/140\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0124 - acc: 0.9874 - val_loss: 0.0618 - val_acc: 0.9357\n",
            "Epoch 134/140\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0120 - acc: 0.9878 - val_loss: 0.0610 - val_acc: 0.9371\n",
            "Epoch 135/140\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0627 - val_acc: 0.9351\n",
            "Epoch 136/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0614 - val_acc: 0.9367\n",
            "Epoch 137/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0626 - val_acc: 0.9347\n",
            "Epoch 138/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0122 - acc: 0.9875 - val_loss: 0.0629 - val_acc: 0.9349\n",
            "Epoch 139/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0120 - acc: 0.9879 - val_loss: 0.0613 - val_acc: 0.9371\n",
            "Epoch 140/140\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0638 - val_acc: 0.9337\n",
            "Train on 102124 samples, validate on 25532 samples\n",
            "Epoch 1/150\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0547 - acc: 0.9314 - val_loss: 0.0500 - val_acc: 0.9375\n",
            "Epoch 2/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0443 - acc: 0.9452 - val_loss: 0.0493 - val_acc: 0.9380\n",
            "Epoch 3/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0397 - acc: 0.9515 - val_loss: 0.0491 - val_acc: 0.9402\n",
            "Epoch 4/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0360 - acc: 0.9564 - val_loss: 0.0490 - val_acc: 0.9395\n",
            "Epoch 5/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0328 - acc: 0.9615 - val_loss: 0.0509 - val_acc: 0.9391\n",
            "Epoch 6/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0302 - acc: 0.9647 - val_loss: 0.0516 - val_acc: 0.9376\n",
            "Epoch 7/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0278 - acc: 0.9679 - val_loss: 0.0518 - val_acc: 0.9376\n",
            "Epoch 8/150\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0259 - acc: 0.9703 - val_loss: 0.0527 - val_acc: 0.9379\n",
            "Epoch 9/150\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0247 - acc: 0.9720 - val_loss: 0.0527 - val_acc: 0.9380\n",
            "Epoch 10/150\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0230 - acc: 0.9743 - val_loss: 0.0558 - val_acc: 0.9344\n",
            "Epoch 11/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0221 - acc: 0.9756 - val_loss: 0.0554 - val_acc: 0.9354\n",
            "Epoch 12/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0212 - acc: 0.9766 - val_loss: 0.0548 - val_acc: 0.9368\n",
            "Epoch 13/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0206 - acc: 0.9777 - val_loss: 0.0567 - val_acc: 0.9345\n",
            "Epoch 14/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0200 - acc: 0.9780 - val_loss: 0.0574 - val_acc: 0.9337\n",
            "Epoch 15/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0192 - acc: 0.9791 - val_loss: 0.0573 - val_acc: 0.9343\n",
            "Epoch 16/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0189 - acc: 0.9796 - val_loss: 0.0558 - val_acc: 0.9375\n",
            "Epoch 17/150\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0189 - acc: 0.9794 - val_loss: 0.0570 - val_acc: 0.9358\n",
            "Epoch 18/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0179 - acc: 0.9807 - val_loss: 0.0575 - val_acc: 0.9358\n",
            "Epoch 19/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0178 - acc: 0.9809 - val_loss: 0.0574 - val_acc: 0.9355\n",
            "Epoch 20/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0171 - acc: 0.9817 - val_loss: 0.0573 - val_acc: 0.9359\n",
            "Epoch 21/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0169 - acc: 0.9819 - val_loss: 0.0569 - val_acc: 0.9369\n",
            "Epoch 22/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0171 - acc: 0.9817 - val_loss: 0.0579 - val_acc: 0.9348\n",
            "Epoch 23/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0171 - acc: 0.9817 - val_loss: 0.0578 - val_acc: 0.9366\n",
            "Epoch 24/150\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0164 - acc: 0.9823 - val_loss: 0.0581 - val_acc: 0.9360\n",
            "Epoch 25/150\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0157 - acc: 0.9836 - val_loss: 0.0580 - val_acc: 0.9366\n",
            "Epoch 26/150\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0160 - acc: 0.9830 - val_loss: 0.0589 - val_acc: 0.9349\n",
            "Epoch 27/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0168 - acc: 0.9818 - val_loss: 0.0592 - val_acc: 0.9355\n",
            "Epoch 28/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0155 - acc: 0.9836 - val_loss: 0.0602 - val_acc: 0.9335\n",
            "Epoch 29/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0154 - acc: 0.9837 - val_loss: 0.0595 - val_acc: 0.9350\n",
            "Epoch 30/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0155 - acc: 0.9837 - val_loss: 0.0588 - val_acc: 0.9359\n",
            "Epoch 31/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0155 - acc: 0.9835 - val_loss: 0.0589 - val_acc: 0.9367\n",
            "Epoch 32/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0150 - acc: 0.9843 - val_loss: 0.0585 - val_acc: 0.9370\n",
            "Epoch 33/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0152 - acc: 0.9840 - val_loss: 0.0609 - val_acc: 0.9339\n",
            "Epoch 34/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0148 - acc: 0.9845 - val_loss: 0.0606 - val_acc: 0.9341\n",
            "Epoch 35/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0149 - acc: 0.9843 - val_loss: 0.0590 - val_acc: 0.9363\n",
            "Epoch 36/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9846 - val_loss: 0.0626 - val_acc: 0.9318\n",
            "Epoch 37/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0144 - acc: 0.9852 - val_loss: 0.0614 - val_acc: 0.9336\n",
            "Epoch 38/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0612 - val_acc: 0.9338\n",
            "Epoch 39/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0146 - acc: 0.9848 - val_loss: 0.0601 - val_acc: 0.9356\n",
            "Epoch 40/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0149 - acc: 0.9843 - val_loss: 0.0597 - val_acc: 0.9363\n",
            "Epoch 41/150\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0144 - acc: 0.9851 - val_loss: 0.0606 - val_acc: 0.9346\n",
            "Epoch 42/150\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0141 - acc: 0.9852 - val_loss: 0.0605 - val_acc: 0.9350\n",
            "Epoch 43/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0148 - acc: 0.9845 - val_loss: 0.0607 - val_acc: 0.9352\n",
            "Epoch 44/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9851 - val_loss: 0.0614 - val_acc: 0.9343\n",
            "Epoch 45/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0142 - acc: 0.9853 - val_loss: 0.0598 - val_acc: 0.9360\n",
            "Epoch 46/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0139 - acc: 0.9856 - val_loss: 0.0605 - val_acc: 0.9355\n",
            "Epoch 47/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0141 - acc: 0.9853 - val_loss: 0.0600 - val_acc: 0.9362\n",
            "Epoch 48/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0143 - acc: 0.9850 - val_loss: 0.0614 - val_acc: 0.9347\n",
            "Epoch 49/150\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0139 - acc: 0.9855 - val_loss: 0.0630 - val_acc: 0.9327\n",
            "Epoch 50/150\n",
            "102124/102124 [==============================] - 6s 57us/step - loss: 0.0139 - acc: 0.9856 - val_loss: 0.0627 - val_acc: 0.9327\n",
            "Epoch 51/150\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0622 - val_acc: 0.9337\n",
            "Epoch 52/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0140 - acc: 0.9854 - val_loss: 0.0600 - val_acc: 0.9366\n",
            "Epoch 53/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0612 - val_acc: 0.9350\n",
            "Epoch 54/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0137 - acc: 0.9859 - val_loss: 0.0607 - val_acc: 0.9354\n",
            "Epoch 55/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0135 - acc: 0.9860 - val_loss: 0.0608 - val_acc: 0.9355\n",
            "Epoch 56/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0139 - acc: 0.9856 - val_loss: 0.0608 - val_acc: 0.9357\n",
            "Epoch 57/150\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0136 - acc: 0.9859 - val_loss: 0.0605 - val_acc: 0.9359\n",
            "Epoch 58/150\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0624 - val_acc: 0.9336\n",
            "Epoch 59/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0135 - acc: 0.9861 - val_loss: 0.0600 - val_acc: 0.9372\n",
            "Epoch 60/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0135 - acc: 0.9859 - val_loss: 0.0627 - val_acc: 0.9335\n",
            "Epoch 61/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9861 - val_loss: 0.0610 - val_acc: 0.9353\n",
            "Epoch 62/150\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0133 - acc: 0.9864 - val_loss: 0.0613 - val_acc: 0.9351\n",
            "Epoch 63/150\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0138 - acc: 0.9857 - val_loss: 0.0644 - val_acc: 0.9317\n",
            "Epoch 64/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0138 - acc: 0.9857 - val_loss: 0.0606 - val_acc: 0.9360\n",
            "Epoch 65/150\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0136 - acc: 0.9858 - val_loss: 0.0604 - val_acc: 0.9367\n",
            "Epoch 66/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0137 - acc: 0.9858 - val_loss: 0.0610 - val_acc: 0.9360\n",
            "Epoch 67/150\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0133 - acc: 0.9861 - val_loss: 0.0622 - val_acc: 0.9343\n",
            "Epoch 68/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0134 - acc: 0.9863 - val_loss: 0.0604 - val_acc: 0.9366\n",
            "Epoch 69/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0626 - val_acc: 0.9338\n",
            "Epoch 70/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0132 - acc: 0.9865 - val_loss: 0.0610 - val_acc: 0.9355\n",
            "Epoch 71/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0661 - val_acc: 0.9300\n",
            "Epoch 72/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0135 - acc: 0.9860 - val_loss: 0.0621 - val_acc: 0.9346\n",
            "Epoch 73/150\n",
            "102124/102124 [==============================] - 5s 51us/step - loss: 0.0132 - acc: 0.9866 - val_loss: 0.0617 - val_acc: 0.9349\n",
            "Epoch 74/150\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0131 - acc: 0.9865 - val_loss: 0.0611 - val_acc: 0.9353\n",
            "Epoch 75/150\n",
            "102124/102124 [==============================] - 5s 49us/step - loss: 0.0134 - acc: 0.9862 - val_loss: 0.0616 - val_acc: 0.9356\n",
            "Epoch 76/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0133 - acc: 0.9863 - val_loss: 0.0626 - val_acc: 0.9340\n",
            "Epoch 77/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0130 - acc: 0.9867 - val_loss: 0.0618 - val_acc: 0.9355\n",
            "Epoch 78/150\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0604 - val_acc: 0.9369\n",
            "Epoch 79/150\n",
            "102124/102124 [==============================] - 5s 52us/step - loss: 0.0134 - acc: 0.9861 - val_loss: 0.0613 - val_acc: 0.9359\n",
            "Epoch 80/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0625 - val_acc: 0.9348\n",
            "Epoch 81/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0132 - acc: 0.9864 - val_loss: 0.0618 - val_acc: 0.9355\n",
            "Epoch 82/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0131 - acc: 0.9865 - val_loss: 0.0620 - val_acc: 0.9350\n",
            "Epoch 83/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0129 - acc: 0.9867 - val_loss: 0.0617 - val_acc: 0.9353\n",
            "Epoch 84/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0129 - acc: 0.9867 - val_loss: 0.0641 - val_acc: 0.9321\n",
            "Epoch 85/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0130 - acc: 0.9866 - val_loss: 0.0635 - val_acc: 0.9331\n",
            "Epoch 86/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0130 - acc: 0.9866 - val_loss: 0.0623 - val_acc: 0.9338\n",
            "Epoch 87/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0128 - acc: 0.9870 - val_loss: 0.0611 - val_acc: 0.9361\n",
            "Epoch 88/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0606 - val_acc: 0.9364\n",
            "Epoch 89/150\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0634 - val_acc: 0.9332\n",
            "Epoch 90/150\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0615 - val_acc: 0.9355\n",
            "Epoch 91/150\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0624 - val_acc: 0.9346\n",
            "Epoch 92/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9871 - val_loss: 0.0619 - val_acc: 0.9348\n",
            "Epoch 93/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0131 - acc: 0.9864 - val_loss: 0.0636 - val_acc: 0.9331\n",
            "Epoch 94/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0130 - acc: 0.9866 - val_loss: 0.0632 - val_acc: 0.9338\n",
            "Epoch 95/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0126 - acc: 0.9871 - val_loss: 0.0619 - val_acc: 0.9351\n",
            "Epoch 96/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0128 - acc: 0.9869 - val_loss: 0.0632 - val_acc: 0.9338\n",
            "Epoch 97/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0606 - val_acc: 0.9368\n",
            "Epoch 98/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0611 - val_acc: 0.9361\n",
            "Epoch 99/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9870 - val_loss: 0.0633 - val_acc: 0.9338\n",
            "Epoch 100/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9871 - val_loss: 0.0631 - val_acc: 0.9341\n",
            "Epoch 101/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0635 - val_acc: 0.9334\n",
            "Epoch 102/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0131 - acc: 0.9866 - val_loss: 0.0616 - val_acc: 0.9355\n",
            "Epoch 103/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0631 - val_acc: 0.9340\n",
            "Epoch 104/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0628 - val_acc: 0.9343\n",
            "Epoch 105/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0633 - val_acc: 0.9340\n",
            "Epoch 106/150\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0129 - acc: 0.9868 - val_loss: 0.0645 - val_acc: 0.9323\n",
            "Epoch 107/150\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0126 - acc: 0.9872 - val_loss: 0.0638 - val_acc: 0.9335\n",
            "Epoch 108/150\n",
            "102124/102124 [==============================] - 5s 46us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0628 - val_acc: 0.9344\n",
            "Epoch 109/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0126 - acc: 0.9871 - val_loss: 0.0615 - val_acc: 0.9359\n",
            "Epoch 110/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0127 - acc: 0.9869 - val_loss: 0.0625 - val_acc: 0.9349\n",
            "Epoch 111/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0638 - val_acc: 0.9333\n",
            "Epoch 112/150\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0637 - val_acc: 0.9337\n",
            "Epoch 113/150\n",
            "102124/102124 [==============================] - 6s 56us/step - loss: 0.0126 - acc: 0.9870 - val_loss: 0.0642 - val_acc: 0.9329\n",
            "Epoch 114/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0632 - val_acc: 0.9344\n",
            "Epoch 115/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0125 - acc: 0.9872 - val_loss: 0.0641 - val_acc: 0.9333\n",
            "Epoch 116/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0631 - val_acc: 0.9343\n",
            "Epoch 117/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0615 - val_acc: 0.9357\n",
            "Epoch 118/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9873 - val_loss: 0.0627 - val_acc: 0.9347\n",
            "Epoch 119/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0623 - val_acc: 0.9349\n",
            "Epoch 120/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0630 - val_acc: 0.9340\n",
            "Epoch 121/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0122 - acc: 0.9876 - val_loss: 0.0665 - val_acc: 0.9302\n",
            "Epoch 122/150\n",
            "102124/102124 [==============================] - 5s 53us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0645 - val_acc: 0.9326\n",
            "Epoch 123/150\n",
            "102124/102124 [==============================] - 5s 54us/step - loss: 0.0120 - acc: 0.9878 - val_loss: 0.0633 - val_acc: 0.9340\n",
            "Epoch 124/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0629 - val_acc: 0.9343\n",
            "Epoch 125/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0619 - val_acc: 0.9351\n",
            "Epoch 126/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0122 - acc: 0.9876 - val_loss: 0.0617 - val_acc: 0.9360\n",
            "Epoch 127/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0633 - val_acc: 0.9340\n",
            "Epoch 128/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0122 - acc: 0.9876 - val_loss: 0.0625 - val_acc: 0.9349\n",
            "Epoch 129/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0639 - val_acc: 0.9334\n",
            "Epoch 130/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0629 - val_acc: 0.9345\n",
            "Epoch 131/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0122 - acc: 0.9877 - val_loss: 0.0617 - val_acc: 0.9357\n",
            "Epoch 132/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0656 - val_acc: 0.9319\n",
            "Epoch 133/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0639 - val_acc: 0.9336\n",
            "Epoch 134/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0121 - acc: 0.9876 - val_loss: 0.0613 - val_acc: 0.9366\n",
            "Epoch 135/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0122 - acc: 0.9875 - val_loss: 0.0623 - val_acc: 0.9355\n",
            "Epoch 136/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0120 - acc: 0.9878 - val_loss: 0.0630 - val_acc: 0.9348\n",
            "Epoch 137/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0623 - val_acc: 0.9353\n",
            "Epoch 138/150\n",
            "102124/102124 [==============================] - 5s 50us/step - loss: 0.0121 - acc: 0.9877 - val_loss: 0.0616 - val_acc: 0.9363\n",
            "Epoch 139/150\n",
            "102124/102124 [==============================] - 6s 54us/step - loss: 0.0124 - acc: 0.9874 - val_loss: 0.0626 - val_acc: 0.9351\n",
            "Epoch 140/150\n",
            "102124/102124 [==============================] - 6s 55us/step - loss: 0.0125 - acc: 0.9873 - val_loss: 0.0625 - val_acc: 0.9353\n",
            "Epoch 141/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0123 - acc: 0.9874 - val_loss: 0.0630 - val_acc: 0.9350\n",
            "Epoch 142/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0122 - acc: 0.9875 - val_loss: 0.0627 - val_acc: 0.9350\n",
            "Epoch 143/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0124 - acc: 0.9873 - val_loss: 0.0633 - val_acc: 0.9342\n",
            "Epoch 144/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0124 - acc: 0.9872 - val_loss: 0.0626 - val_acc: 0.9351\n",
            "Epoch 145/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0122 - acc: 0.9876 - val_loss: 0.0618 - val_acc: 0.9360\n",
            "Epoch 146/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0119 - acc: 0.9879 - val_loss: 0.0600 - val_acc: 0.9382\n",
            "Epoch 147/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0120 - acc: 0.9877 - val_loss: 0.0618 - val_acc: 0.9361\n",
            "Epoch 148/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0123 - acc: 0.9875 - val_loss: 0.0623 - val_acc: 0.9352\n",
            "Epoch 149/150\n",
            "102124/102124 [==============================] - 5s 48us/step - loss: 0.0120 - acc: 0.9878 - val_loss: 0.0626 - val_acc: 0.9350\n",
            "Epoch 150/150\n",
            "102124/102124 [==============================] - 5s 47us/step - loss: 0.0119 - acc: 0.9879 - val_loss: 0.0612 - val_acc: 0.9371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SSFaCvrqtR_L",
        "colab_type": "code",
        "outputId": "0e930f8f-a440-4a6f-a25e-bd9d09eb3529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "test_conf_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[28177,   493],\n",
              "        [ 1940,  1305]]), array([[27716,   954],\n",
              "        [ 1786,  1459]]), array([[27508,  1162],\n",
              "        [ 1609,  1636]]), array([[27265,  1405],\n",
              "        [ 1610,  1635]]), array([[27357,  1313],\n",
              "        [ 1628,  1617]]), array([[27083,  1587],\n",
              "        [ 1570,  1675]]), array([[26851,  1819],\n",
              "        [ 1567,  1678]]), array([[27213,  1457],\n",
              "        [ 1566,  1679]]), array([[27564,  1106],\n",
              "        [ 1672,  1573]]), array([[26743,  1927],\n",
              "        [ 1497,  1748]]), array([[26309,  2361],\n",
              "        [ 1472,  1773]]), array([[26664,  2006],\n",
              "        [ 1522,  1723]]), array([[26925,  1745],\n",
              "        [ 1545,  1700]]), array([[25301,  3369],\n",
              "        [ 1339,  1906]]), array([[27176,  1494],\n",
              "        [ 1617,  1628]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "metadata": {
        "id": "oB2amAaaxgtV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**DNN Classifier - plot Tuning paramter vs recall **"
      ]
    },
    {
      "metadata": {
        "id": "h9xhlG_QXQeE",
        "colab_type": "code",
        "outputId": "e8c39e16-5af1-4b97-e591-4e0b18d5d680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "from matplotlib.legend_handler import HandlerLine2D\n",
        "line1, = plt.plot(epoch_sizes, test_recalls, 'b', label=\"Test recall\")\n",
        "line2, = plt.plot(epoch_sizes, test_accuracys, 'r', label=\"Test accuracy\")\n",
        "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
        "plt.ylabel('Measure')\n",
        "plt.xlabel('epochs')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAENCAYAAAAVPvJNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBIQQQgiXJCCEEEK4JAEhhBDCpf8HAL3GVHEqd1AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "bTolzMeE1l7b",
        "colab_type": "code",
        "outputId": "aaef3452-2f43-412e-f1b9-c6877f7faefa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "cell_type": "code",
      "source": [
        "cm =  test_conf_matrix[13]\n",
        "plt.clf()\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
        "classNames = ['Non-Toxic','Toxic']\n",
        "plt.title('Confusion Matrix - Test Data')\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "tick_marks = np.arange(len(classNames))\n",
        "plt.xticks(tick_marks, classNames, rotation=45)\n",
        "plt.yticks(tick_marks, classNames)\n",
        "s = [['TN','TP'], ['FP', 'TP']]\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAE7CAYAAAC8FRZ0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qOMvTjsNx7wZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Complex Ensemble Model - AdaBoost Classifier**"
      ]
    },
    {
      "metadata": {
        "id": "svjLNQJVNFv0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "model = AdaBoostClassifier(random_state=5, n_estimators=25, learning_rate=1)\n",
        "\n",
        "model.fit(train_data, y_train)\n",
        "y_pred = model.predict(test_data)\n",
        "\n",
        "adaboost_filename = 'adaboost.pkl'\n",
        "adaboost_model_pkl = open(adaboost_filename, 'wb')\n",
        "pickle.dump(model, adaboost_model_pkl)\n",
        "adaboost_model_pkl.close()\n",
        "model_dict['adaboost'] = model\n",
        "print(\"Model Saved\")\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "model.score(test_data,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xQcTTeWnx-4T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Complex Ensemble Model - Random Forest Classifier**"
      ]
    },
    {
      "metadata": {
        "id": "Rfs6uaR7N1dn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model= RandomForestClassifier(random_state=1, n_estimators=25)\n",
        "\n",
        "randForest_filename = 'randForest.pkl'\n",
        "randForest_model_pkl = open(randForest_filename, 'wb')\n",
        "pickle.dump(model, randForest_model_pkl)\n",
        "randForest_model_pkl.close()\n",
        "model_dict['randForest'] = model\n",
        "print(\"Model Saved\")\n",
        "\n",
        "model.fit(train_data, y_train)\n",
        "y_pred = model.predict(test_data)\n",
        "print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n",
        "model.score(test_data,y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jh2avquoyCoP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Generate ROC Curve/ Confusion Matrix for all the models other than DNN**"
      ]
    },
    {
      "metadata": {
        "id": "OEUugeKjjx6E",
        "colab_type": "code",
        "outputId": "304b4d6e-a98c-48b3-89e9-87bc111eba96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        }
      },
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "\n",
        "# Generate ROC curve for all the models\n",
        "\n",
        "for model_name, model in model_dict.items(): \n",
        "  # calculate the fpr and tpr for all thresholds of the classification\n",
        "  if model_name == 'doc2vec':\n",
        "    continue\n",
        "  elif model_name == 'sequential':\n",
        "    continue\n",
        "    y_pred_test = model.predict(np.array(test_data))\n",
        "  else:\n",
        "    y_pred_test = model.predict(test_data)\n",
        "    \n",
        "  cm=metrics.confusion_matrix(y_test, y_pred_test)\n",
        "  #print(cm)\n",
        "  print(pd.crosstab(y_test, y_pred_test, rownames=['True'], colnames=['Predicted'], margins=True))\n",
        "  plt.clf()\n",
        "  plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
        "  classNames = ['Non-Toxic','Toxic']\n",
        "  plt.title('Confusion Matrix - Test Data')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  tick_marks = np.arange(len(classNames))\n",
        "  plt.xticks(tick_marks, classNames, rotation=45)\n",
        "  plt.yticks(tick_marks, classNames)\n",
        "  s = [['TN','FP'], ['FN', 'TP']]\n",
        "  for i in range(2):\n",
        "      for j in range(2):\n",
        "          plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
        "  \n",
        "  \n",
        "  plt.show()\n",
        "  print(model_name)\n",
        "  accuracy_confusion_model = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0])\n",
        "  precision_confusion_model = (cm[1][1])/(cm[0][1]+cm[1][1])\n",
        "  recall_confusion_model = (cm[1][1])/(cm[1][0]+cm[1][1])\n",
        "  f_measure_confusion_model = 2*(precision_confusion_model*recall_confusion_model)/(precision_confusion_model+recall_confusion_model)\n",
        "\n",
        "  print(\"accuracy is -> \" + str(accuracy_confusion_model))\n",
        "  print(\"precision is -> \" + str(precision_confusion_model))\n",
        "  print(\"recall is -> \" + str(recall_confusion_model))\n",
        "  print(\"f_measure is -> \" + str(f_measure_confusion_model))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted      0    1    All\n",
            "True                        \n",
            "0          28510  160  28670\n",
            "1           2806  439   3245\n",
            "All        31316  599  31915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAE7CAYAAAC8FRZ0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npkioCSE0RUJNCKEp/wetXg0e0FY80AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "lr\n",
            "accuracy is -> 0.907065643114523\n",
            "precision is -> 0.7328881469115192\n",
            "recall is -> 0.1352850539291217\n",
            "f_measure is -> 0.22840790842872005\n",
            "Predicted      0     1    All\n",
            "True                         \n",
            "0          23869  4801  28670\n",
            "1           2158  1087   3245\n",
            "All        26027  5888  31915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAE7CAYAAAC8FRZ0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "ensemble_vote\n",
            "accuracy is -> 0.7819520601597995\n",
            "precision is -> 0.18461277173913043\n",
            "recall is -> 0.3349768875192604\n",
            "f_measure is -> 0.23803788459432826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zowz1GbgyKSc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Generate Confusion Matrix for  DNN**"
      ]
    },
    {
      "metadata": {
        "id": "naS_GlI2O1ki",
        "colab_type": "code",
        "outputId": "ec8eac8f-f182-4cf1-97df-fcfee0390369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "!cp gdrive/My\\ Drive/ALDA\\ Capstone/sequential.pkl .\n",
        "filename = 'sequential.pkl'\n",
        "dt_clf = pickle.load(open(filename, 'rb'))\n",
        "#dt_clf = dnn_models[140]\n",
        "\n",
        "y_pred_test = dt_clf.predict(np.array(test_data))\n",
        "#print(type(y_test))\n",
        "\n",
        "y_pred = []\n",
        "for x in y_pred_test:\n",
        "#   if x[0] > 0.5:\n",
        "#     y_pred.append(1)\n",
        "#   else:\n",
        "#     y_pred.append(0)\n",
        "  y_pred.append(x)\n",
        "    \n",
        "y_pred_test = pd.Series(y_pred)\n",
        "\n",
        "# cm=metrics.confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "# plt.clf()\n",
        "# plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
        "# classNames = ['Negative','Positive']\n",
        "# plt.title('Confusion Matrix - Test Data')\n",
        "# plt.ylabel('True label')\n",
        "# plt.xlabel('Predicted label')\n",
        "# tick_marks = np.arange(len(classNames))\n",
        "# plt.xticks(tick_marks, classNames, rotation=45)\n",
        "# plt.yticks(tick_marks, classNames)\n",
        "# s = [['TN','TP'], ['FP', 'TP']]\n",
        "# for i in range(2):\n",
        "#     for j in range(2):\n",
        "#         plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
        "# plt.show()\n",
        "\n",
        "# accuracy_confusion_model = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[1][1]+cm[0][1]+cm[1][0])\n",
        "# precision_confusion_model = (cm[1][1])/(cm[0][1]+cm[1][1])\n",
        "# recall_confusion_model = (cm[1][1])/(cm[1][0]+cm[1][1])\n",
        "# f_measure_confusion_model = 2*(precision_confusion_model*recall_confusion_model)/(precision_confusion_model+recall_confusion_model)\n",
        "\n",
        "# print(\"accuracy is -> \" + str(accuracy_confusion_model))\n",
        "# print(\"precision is -> \" + str(precision_confusion_model))\n",
        "# print(\"recall is -> \" + str(recall_confusion_model))\n",
        "# print(\"f_measure is -> \" + str(f_measure_confusion_model))\n",
        "\n",
        "probs = dt_clf.predict_proba(np.array(test_data))\n",
        "#probs = model.predict_proba(y_test)\n",
        "\n",
        "# preds = probs[:,1]\n",
        "# print(preds)\n",
        "print(probs)\n",
        "fpr, tpr, threshold = metrics.roc_curve(y_test, probs)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Receiver Operating Characteristic for {}'.format('DNN'))\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0000000e+00]\n",
            " [0.0000000e+00]\n",
            " [1.0000000e+00]\n",
            " ...\n",
            " [3.9102087e-14]\n",
            " [5.5391797e-13]\n",
            " [1.3892348e-03]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}